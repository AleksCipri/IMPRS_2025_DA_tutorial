{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec25be4d",
   "metadata": {},
   "source": [
    "Probing the Limits of Domain Adaptation\n",
    "---\n",
    "In the last tutorial, we saw an instance of distance-based and adversarial domain adaptation (DA) techniques working to improve the generalization of a NN trained on MNIST and evaluated on MNIST-M. We will now move to a more physical dataset of galaxy morphologies, with the differences between source and target domain due to fundamentally different imaging devices. We will be using the Sinkhorn divergence as our distance metric for DA. \n",
    "\n",
    "<img src=\"https://snehjp2.github.io/images/gaussian_meme.png\" alt=\"gaussian meme\" width=\"600\"/>\n",
    "\n",
    "\n",
    "The particular focus here will be playing around with hyperparameters, illustrating the success and failure points of DA and the importance of finding the right sweet-spot for training. We will end with more sophisticated techniques for implementing DA that can automate-away the need for hyperparameter tuning. We'll start with the same code imports as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1cfbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch related inputs\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "import copy\n",
    "\n",
    "# miscellaneous\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Optional\n",
    "import os\n",
    "import random\n",
    "\n",
    "# geomloss provides distance measure that are torch/CUDA compatible. \n",
    "# uncomment line below to install it, if you don't have it installed.\n",
    "\n",
    "# !pip install -q geomloss\n",
    "\n",
    "def set_all_seeds(seed=0):\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # For Python's hash seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Optional: enforce deterministic algorithms where possible\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    \n",
    "set_all_seeds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374197f",
   "metadata": {},
   "source": [
    "Problem Setup\n",
    "---\n",
    "\n",
    "Our problem setup follows similarly from Tutorial 1.\n",
    "\n",
    "- **Neural Network**:  \n",
    "Let $f_\\theta$ denote a neural network classifier with parameters $\\theta$, which maps an input image $x \\in \\mathbb{R}^n$ to predicted class probabilities $\\hat{y} = f_\\theta(x) \\in \\mathbb{R}^K$. The network is decomposed into a feature extractor $\\phi_\\theta : \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ and a classifier head $g_\\theta : \\mathbb{R}^d \\rightarrow \\mathbb{R}^K$, such that $f_\\theta(x) = \\text{softmax}(g_\\theta(\\phi_\\theta(x)))$.  \n",
    "We define the **latent representation** $z = \\phi_\\theta(x)$ as the output of the final hidden layer (before the logits). This representation (also called the *latent vector* or *latent space*) will be used for domain alignment.\n",
    "\n",
    "- **Source domain dataset**:  \n",
    "$\\mathcal{D}_s = \\{(x_s^{(i)}, y_s^{(i)})\\}_{i=1}^{N_s}$,  \n",
    "where $x_s^{(i)} \\sim p_s(x)$ are galaxy morphology observations from SDSS, and $y_s^{(i)} \\in \\{0, 1, \\dots, 9\\}$ are the corresponding morphology labels.\n",
    "\n",
    "- **Target domain dataset**:  \n",
    "$\\mathcal{D}_t = \\{x_t^{(j)}\\}_{j=1}^{N_t}$,  \n",
    "where $x_t^{(j)} \\sim p_t(x)$ are unlabeled galaxy morphology observations from DESI. The included morphologies are the same in both datasets. \n",
    "We assume the label distributions are aligned, i.e., $p_s(y|x) = p_t(y|x)$, but the input distributions differ: $p_s(x) \\neq p_t(x)$.  \n",
    "Our goal is to adapt $f_\\theta$ using **only labeled source data** and **unlabeled target data**, so that it performs well on the target domain.\n",
    "\n",
    "- **Classification loss**:  \n",
    "On the source domain, we minimize the supervised cross-entropy loss between predicted and true labels:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CE}}(\\theta) = -\\frac{1}{N_s} \\sum_{i=1}^{N_s} \\log f_\\theta^{(y_s^{(i)})}(x_s^{(i)}),\n",
    "$$\n",
    "\n",
    "where $f_\\theta^{(k)}(x)$ denotes the predicted probability for class $k$, and $K = 10$ is the number of classes.\n",
    "\n",
    "---\n",
    "Let's start by defining our necessary ingredients, starting with the data. We will download the [Galaxy Zoo Evo Dataset](https://huggingface.co/datasets/mwalmsley/gz_evo) from [this link](https://zenodo.org/uploads/14583107). The dataset is 2.35 GB in size; make sure to place it in the `data/` directory for the code below to run correctly. Galaxy Zoo (GZ) is a citizen science project that labels galaxy images through online participation. GZ Evo combines labeled image datasets across several surveys and iterations of GZ. Within GZ Evo, we use the GZ2 Dataset from the Sloan Digital Sky Survey (SDSS) [Willett et al., 2013] as the source, and a GZ Dark Energy Spectroscopic Instrument (DESI) dataset that combines observations from the DESI Imaging Surveys (DECals, MzLS, BASS, DES) as the target domain.\n",
    "\n",
    "Recall that images are typically normalized to be in the range [-1,1] for gradient stabilitiy during training. This is done via [z-score normalization](https://en.wikipedia.org/wiki/Standard_score). I've precalculated the means and standard deviations of the datasets for you. Feel free to check this yourself! This is an often not-discussed/overlooked aspect of training NNs, but data normalization is **immensely important**. Lets start by defining our dataset class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300e09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GZEvo(Dataset):\n",
    "    \"\"\"Dataset class for the Galaxy Zoo Evolution dataset.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input data.\n",
    "        output_path (Optional[str], optional): Path to the output data. Defaults to None.\n",
    "        transform (Optional[Callable], optional): Transform to apply to the data. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        output_path: Optional[str] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.transform = transform\n",
    "\n",
    "        try:\n",
    "            self.img = np.load(self.input_path)\n",
    "            self.label = np.load(self.output_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Error loading data from {input_path} and {output_path}: {e}\"\n",
    "            )\n",
    "\n",
    "        self.length = len(self.img)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = self.img[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.tensor(self.label[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b283ad7",
   "metadata": {},
   "source": [
    "Like with MNIST-M, we need to be careful about image normalization. I've precalculated the dataset statistics for you again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_size = 10000\n",
    "train_subset_size = int(data_subset_size * 0.8)\n",
    "test_subset_size = data_subset_size - train_subset_size\n",
    "\n",
    "sdss_mean = (0.0439, 0.0388, 0.0289)\n",
    "sdss_std = (0.0816, 0.0687, 0.0590)\n",
    "desi_mean = (0.1040, 0.0971, 0.0951)\n",
    "desi_std = (0.0835, 0.0789, 0.0754)\n",
    "\n",
    "sdss_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=sdss_mean, std=sdss_std),\n",
    "                transforms.Resize((28, 28)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "desi_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=desi_mean, std=desi_std),\n",
    "                transforms.Resize((28, 28)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "source_train_dataset = GZEvo(\n",
    "    input_path=\"./data/galaxy_dataset/galaxy_dataset/train/x_train_sdss.npy\",\n",
    "    output_path=\"./data/galaxy_dataset/galaxy_dataset/train/y_train_sdss.npy\",\n",
    "    transform=sdss_transform\n",
    ")\n",
    "\n",
    "target_train_dataset = GZEvo(\n",
    "    input_path=\"./data/galaxy_dataset/galaxy_dataset/train/x_train_desi.npy\",\n",
    "    output_path=\"./data/galaxy_dataset/galaxy_dataset/train/y_train_desi.npy\",\n",
    "    transform=desi_transform\n",
    ")\n",
    "\n",
    "source_test_dataset = GZEvo(\n",
    "    input_path=\"./data/galaxy_dataset/galaxy_dataset/test/x_test_sdss.npy\",\n",
    "    output_path=\"./data/galaxy_dataset/galaxy_dataset/test/y_test_sdss.npy\",\n",
    "    transform=sdss_transform\n",
    ")\n",
    "\n",
    "target_test_dataset = GZEvo(\n",
    "    input_path=\"./data/galaxy_dataset/galaxy_dataset/test/x_test_desi.npy\",\n",
    "    output_path=\"./data/galaxy_dataset/galaxy_dataset/test/y_test_desi.npy\",\n",
    "    transform=desi_transform\n",
    ")\n",
    "\n",
    "source_train_indices = torch.randperm(len(source_train_dataset))[:train_subset_size]\n",
    "source_test_indices = torch.randperm(len(source_test_dataset))[:test_subset_size]\n",
    "target_train_indices = torch.randperm(len(target_train_dataset))[:train_subset_size]\n",
    "target_test_indices = torch.randperm(len(target_test_dataset))[:test_subset_size]\n",
    "\n",
    "source_train_dataset = Subset(source_train_dataset, source_train_indices)\n",
    "source_test_dataset = Subset(source_test_dataset, source_test_indices)\n",
    "target_train_dataset = Subset(target_train_dataset, target_train_indices)\n",
    "target_test_dataset = Subset(target_test_dataset, target_test_indices)\n",
    "\n",
    "source_train_loader = DataLoader(\n",
    "    source_train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "target_train_loader = DataLoader(\n",
    "    target_train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "source_test_loader = DataLoader(\n",
    "    source_test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "target_test_loader = DataLoader(\n",
    "    target_test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61380cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([ 1.7008e-03,  1.4441e-03, -6.5291e-05], dtype=torch.float64)\n",
      "Std: tensor([0.9296, 0.9277, 0.9186], dtype=torch.float64)\n",
      "Mean: tensor([0.0016, 0.0028, 0.0023], dtype=torch.float64)\n",
      "Std: tensor([0.9124, 0.9235, 0.9208], dtype=torch.float64)\n",
      "Mean: tensor([-0.0026,  0.0031,  0.0042], dtype=torch.float64)\n",
      "Std: tensor([0.9298, 0.9289, 0.9238], dtype=torch.float64)\n",
      "Mean: tensor([0.0095, 0.0101, 0.0090], dtype=torch.float64)\n",
      "Std: tensor([0.9182, 0.9290, 0.9254], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def compute_mean_std(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for imgs, _ in loader:\n",
    "        # imgs shape: (B, C, H, W)\n",
    "        batch_samples = imgs.size(0)\n",
    "        imgs = imgs.view(batch_samples, imgs.size(1), -1)  # (B, C, H*W)\n",
    "        mean += imgs.mean(2).sum(0)\n",
    "        std += imgs.std(2).sum(0)\n",
    "        n_samples += batch_samples\n",
    "\n",
    "    mean /= n_samples\n",
    "    std /= n_samples\n",
    "    return mean, std\n",
    "\n",
    "# Example: Compute for source training set\n",
    "mean, std = compute_mean_std(source_train_dataset)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n",
    "mean, std = compute_mean_std(target_train_dataset)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n",
    "mean, std = compute_mean_std(source_test_dataset)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n",
    "mean, std = compute_mean_std(target_test_dataset)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4388b0",
   "metadata": {},
   "source": [
    "Now that the data is downloaded, lets just sanity check that the images look like what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bd909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_dict = {'barred_spiral': 0, \n",
    "               'edge_on_disk': 1, \n",
    "               'featured_without_bar_or_spiral': 2, \n",
    "               'smooth_cigar': 3, \n",
    "               'smooth_round': 4, \n",
    "               'unbarred_spiral': 5\n",
    "            }\n",
    "labels_dict = {v: k for k, v in labels_dict.items()}  # Reverse the dictionary for plotting\n",
    "\n",
    "def get_examples_by_label(dataset):\n",
    "    label_to_img = {}\n",
    "    for img, label in dataset:\n",
    "        label = int(label)\n",
    "        if label not in label_to_img:\n",
    "            label_to_img[label] = img\n",
    "        if len(label_to_img) == 6:\n",
    "            break\n",
    "    return label_to_img\n",
    "\n",
    "# Undo normalization\n",
    "def unnormalize(img, mean, std):\n",
    "    img = img.clone()\n",
    "    for c in range(img.shape[0]):\n",
    "        img[c] = img[c] * std[c] + mean[c]\n",
    "    return img\n",
    "\n",
    "# Collect examples\n",
    "sdss_galaxy = get_examples_by_label(source_train_dataset)\n",
    "desi_galaxy = get_examples_by_label(target_train_dataset)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 6, figsize=(10, 4))\n",
    "\n",
    "for i in range(6):\n",
    "    # SDSS (top row)\n",
    "    image_sdss = unnormalize(sdss_galaxy[i], sdss_mean, sdss_std)\n",
    "    axes[0, i].imshow(image_sdss.permute(1, 2, 0).numpy())\n",
    "    # use labels from labels_dict\n",
    "    axes[0, i].set_title(f\"{labels_dict[i]}\", fontsize=10)\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    # DESI (bottom row)\n",
    "    image_desi = unnormalize(desi_galaxy[i], desi_mean, desi_std)\n",
    "    axes[1, i].imshow(image_desi.permute(1, 2, 0).numpy())\n",
    "    axes[1, i].set_title(f\"{labels_dict[i]}\", fontsize=10)\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"SDSS\", fontsize=12)\n",
    "axes[1, 0].set_ylabel(\"DESI\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9ce8c",
   "metadata": {},
   "source": [
    "Images look good, now lets just check that the shapes are the same for training, and that the z-score normalization statistics look ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3feb0521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDSS image shape: torch.Size([1, 3, 28, 28])\n",
      "DESI image shape: torch.Size([1, 3, 28, 28])\n",
      "SDSS image stats: tensor(0.3389, dtype=torch.float64) tensor(1.2147, dtype=torch.float64)\n",
      "DESI image stats: tensor(0.0340, dtype=torch.float64) tensor(1.0110, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Get a sample image from each dataset\n",
    "sdss_image = next(iter(DataLoader(source_test_dataset, batch_size=1)))[0]\n",
    "desi_image = next(iter(DataLoader(source_train_dataset, batch_size=1)))[0]\n",
    "\n",
    "# Print the shapes\n",
    "print(\"SDSS image shape:\", sdss_image.shape) # should be (3, 28, 28)\n",
    "print(\"DESI image shape:\", desi_image.shape) # should be (3, 28, 28)\n",
    "print(\"SDSS image stats:\", torch.mean(sdss_image), torch.std(sdss_image)) # should be close to 0 and 1\n",
    "print(\"DESI image stats:\", torch.mean(desi_image), torch.std(desi_image)) # should be close to 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9987e",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's define our model. We will use the same CNN architecture from the previous tutorial. We've added [layer normalization](https://arxiv.org/abs/1607.06450) before returning the latent space to help stabalize the representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32206a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten_dim = 32 * 14 * 14  # for input images of shape (3, 50, 50)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)  # apply to latent vector\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))\n",
    "        z = x.view(-1, self.flatten_dim)\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = self.ln1(z)  # apply LayerNorm to the latent vector\n",
    "        out = self.fc2(z)\n",
    "        return out, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c027e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:03<00:11,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4]  Loss: 1.1451  Accuracy: 57.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:07<00:07,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4]  Loss: 0.7647  Accuracy: 71.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:11<00:03,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4]  Loss: 0.6944  Accuracy: 73.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:15<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/4]  Loss: 0.6248  Accuracy: 76.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "CE_only_model = CNN(num_classes=6).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(CE_only_model.parameters(), lr=1e-3)\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "# Training loop\n",
    "num_epochs = 4\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    CE_only_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in source_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = CE_only_model(images) ## not using latent z for now\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model = copy.deepcopy(CE_only_model.state_dict())\n",
    "\n",
    "\n",
    "        # Track training stats\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]  Loss: {running_loss/len(source_train_loader):.4f}  Accuracy: {100*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2236f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (SDSS): 73.10%\n",
      "Accuracy on target domain (DESI): 61.05%\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.float()\n",
    "            outputs, _ = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Test on source domain (MNIST)\n",
    "CE_only_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(CE_only_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (SDSS): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (MNIST-M)\n",
    "target_accuracy = test_model(CE_only_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (DESI): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a602dc",
   "metadata": {},
   "source": [
    "We see that the model has a slight % discrepancy between the source and target domain. Indeed, despite the galaxy observations coming from fundamentally different imaging instruments, the images dont look *too* different. Can you think about why the two datasets look similar despite coming from SDSS and DESI? Think about the photometric filters used in both surveys, and their wavelength coverage. \n",
    "\n",
    "*Stretch Question:* Would you expect galaxy observations from JWST and DESI to look similar? What would be different about constructing a model that generalizes between SDSS and JWST observations? What kind of domain shift would this be?\n",
    "\n",
    "Now, lets move to domain adaptation and see if we can shrink this performance gap between $\\mathcal{D}_s$ and $\\mathcal{D}_t$. We will again use `geomloss`, but this time using the Sinkhorn Divergence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d305f17",
   "metadata": {},
   "source": [
    "The Sinkhorn Divergence\n",
    "---\n",
    "\n",
    "The optimal transport (OT) distance is distance measure much like MMD. One notable problem with MMD distances is the user-specified choice of kernel function $k$. There are several principled kernel options (e.g., Gaussian, Lapllacian, linear, etc.) with the optimal choice of kernel determined through extensive experimentation. A single kernel may not be optimal, either, and a superposition of kernels may be needed for optimal domain alignment. OT distances avoid this by not requiring a kernel function in computing distances, and have gained popularity in recent years due to *entropic regularization* making the calculation of such distances more manageable. The regularized OT is defined as\n",
    "\n",
    "$$\n",
    "\\text{OT}_\\sigma(\\mu, \\nu) = \\min_{\\gamma \\in U(\\mu, \\nu)} \\left( \\sum_{i,j} \\gamma_{ij} d(z_i, z_j^*)^p + \\sigma H(\\gamma) \\right),\n",
    "$$\n",
    "\n",
    "where $d(z_i, z_j^*)^p$ is the distance between source feature $z_i$ and target feature $z_j^*$. When $p = 1$, this distance becomes the Earth Mover’s Distance [Rubner et al., 1998], and when \\( p = 2 \\), it becomes the quadratic Wasserstein distance.\n",
    "\n",
    "The transport plan $\\gamma \\in U(\\mu, \\nu)$ is a joint probability distribution between $\\mu$ and $\\nu$, where the set of admissible transport plans $U(\\mu, \\nu)$ is defined by the marginal constraints:\n",
    "\n",
    "$$\n",
    "\\sum_j \\gamma_{ij} = \\mu_i, \\quad \\sum_i \\gamma_{ij} = \\nu_j. \\tag{4}\n",
    "$$\n",
    "\n",
    "Much of the expense in the regularized OT problem is finding measures that satisfy these constraints. The entropy $H(\\gamma) = -\\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij}$ regularizes the transport plan $\\gamma$, and $\\sigma$ controls the regularization strength (this is sometimes referred to as the \"blur\" parameter). One limitation of $\\text{OT}_\\sigma$ is that $\\text{OT}_\\sigma(\\mu, \\mu) \\ne 0$, implying a non-zero cost even when transporting a distribution to itself, leading to bias in the measure.\n",
    "\n",
    "To correct this bias, the Sinkhorn divergence $S_\\sigma(\\mu, \\nu)$, defined as\n",
    "\n",
    "$$\n",
    "S_\\sigma(\\mu, \\nu) = \\text{OT}_\\sigma(\\mu, \\nu) - \\frac{1}{2} \\text{OT}_\\sigma(\\mu, \\mu) - \\frac{1}{2} \\text{OT}_\\sigma(\\nu, \\nu), \\tag{5}\n",
    "$$\n",
    "\n",
    "can compensate for the bias in $\\text{OT}_\\sigma$ [Feydy et al., 2018]. As $\\sigma \\to 0$, $S_\\sigma(\\mu, \\nu)$ converges to the (biased) optimal transport $\\text{OT}_0$, and as $\\sigma \\to \\infty$, it interpolates towards MMD loss [Feydy et al., 2018]. For small values of $\\sigma$, an unbiased transport plan that still enjoys the benefits of OT-based distances can be constructed. We're now ready for our full loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{CE}}(y_s, \\hat{y}_s) + \\lambda \\cdot S_\\sigma(z, z^*),\n",
    "$$\n",
    "\n",
    "where $\\lambda$ dictates the strength of the DA loss term.\n",
    "\n",
    "A few understanding checks before we move forward:\n",
    "* Double check that the Sinkhorn Divergence is unbiased: $S_\\sigma(\\mu, \\mu) = S_\\sigma(\\nu, \\nu) = 0$. What would happen if you performed DA with a biased measure ($\\text{OT}(\\mu, \\nu)$)?\n",
    "* There is one hyperparameter here. What is the reason for having $\\sigma \\to 0$ vs. $\\sigma \\to \\infty$? Think also about computational complexity / speed.\n",
    "* What would happen when training with $\\lambda \\to 0$. What about the opposite, when $\\lambda \\gg 1$?\n",
    "\n",
    "Our focus for these next code blocks will be the $\\lambda$ term and one technique to finding the optimal balance between the primary learning objective and DA. Lets start with $\\lambda \\to 0$. We will use a fixed value of $\\sigma = 10$; this is still nearly $\\mathcal{O}(1)$, but no so small that the Sinkhorn iterations become very expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c903675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<00:58,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], CE Loss: 1.1967, DA Loss: 0.2724 Source Acc: 54.46%, Target Acc: 48.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:13<00:53,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], CE Loss: 0.7960, DA Loss: 1.5491 Source Acc: 70.35%, Target Acc: 55.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:20<00:47,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], CE Loss: 0.7173, DA Loss: 2.5587 Source Acc: 73.04%, Target Acc: 57.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:26<00:39,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], CE Loss: 0.6835, DA Loss: 2.7770 Source Acc: 74.29%, Target Acc: 59.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:32<00:32,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], CE Loss: 0.6238, DA Loss: 3.2094 Source Acc: 76.04%, Target Acc: 58.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:38<00:25,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], CE Loss: 0.5959, DA Loss: 3.6147 Source Acc: 77.14%, Target Acc: 58.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:44<00:18,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], CE Loss: 0.5513, DA Loss: 3.8574 Source Acc: 79.42%, Target Acc: 58.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:51<00:12,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], CE Loss: 0.4970, DA Loss: 4.0979 Source Acc: 80.96%, Target Acc: 58.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:57<00:06,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], CE Loss: 0.4439, DA Loss: 4.7558 Source Acc: 83.04%, Target Acc: 58.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:03<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], CE Loss: 0.3964, DA Loss: 5.3365 Source Acc: 84.69%, Target Acc: 59.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from geomloss import SamplesLoss\n",
    "\n",
    "num_epochs = 10\n",
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "DA_model = CNN().to(device)  # CNN should return (logits, z)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(DA_model.parameters(), lr=5e-3)\n",
    "lambda_mmd = 0.0001\n",
    "geomloss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=10) # GeomLoss with Sinkhorn distance, p=2, blur ~ σ; tune as needed\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# --- Training ---\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    DA_model.train()\n",
    "    running_loss, correct_s, correct_t, total_s, total_t = 0.0, 0, 0, 0, 0\n",
    "    running_DA, running_CE = 0.0, 0.0\n",
    "\n",
    "    for (source_imgs, source_labels), (target_imgs, target_labels) in zip(source_train_loader, target_train_loader):\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs, target_labels = target_imgs.to(device), target_labels.to(device)\n",
    "        source_imgs, target_imgs = source_imgs.float(), target_imgs.float()\n",
    "\n",
    "        # --- Concatenate images and forward ---\n",
    "        combined_imgs = torch.cat([source_imgs, target_imgs], dim=0)\n",
    "        logits, z = DA_model(combined_imgs)\n",
    "\n",
    "        # --- Split latent vectors and outputs ---\n",
    "        z_s, z_t = z[:source_imgs.size(0)], z[source_imgs.size(0):]\n",
    "        logits_s = logits[:source_imgs.size(0)]\n",
    "        logits_t = logits[source_imgs.size(0):]\n",
    "\n",
    "        # --- Compute losses ---\n",
    "        ce_loss = criterion(logits_s, source_labels)\n",
    "        mmd_loss = geomloss_fn(z_s, z_t)  # ← GeomLoss replaces MMD here\n",
    "        total_loss = ce_loss + lambda_mmd * mmd_loss\n",
    "        \n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_model = copy.deepcopy(DA_model.state_dict())\n",
    "\n",
    "\n",
    "        # --- Backpropagation ---\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Metrics ---\n",
    "        running_loss += total_loss.item()\n",
    "        running_CE += ce_loss.item()\n",
    "        running_DA += mmd_loss.item()\n",
    "        _, predicted_s = logits_s.max(1)\n",
    "        total_s += source_labels.size(0)\n",
    "        correct_s += predicted_s.eq(source_labels).sum().item()\n",
    "        _, predicted_t = logits_t.max(1)\n",
    "        total_t += target_labels.size(0)\n",
    "        correct_t += predicted_t.eq(target_labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CE Loss: {running_CE/len(source_train_loader):.4f}, DA Loss: {running_DA/len(source_train_loader):.4f} Source Acc: {100*correct_s/total_s:.2f}%, Target Acc: {100*correct_t/total_t:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d46661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (MNIST): 73.10%\n",
      "Accuracy on target domain (MNIST-M): 61.90%\n"
     ]
    }
   ],
   "source": [
    "# Test on source domain (SDSS)\n",
    "DA_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(DA_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (MNIST): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (DESI)\n",
    "target_accuracy = test_model(DA_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (MNIST-M): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f391134",
   "metadata": {},
   "source": [
    "Ok, so virtually no improvement here. In this case, the DA term isn't contributing meaningfully to the loss landscape. A reasonable guess would be that $\\lambda$ should be less than 1, but it is hard to guess a priori whether it should be $\\mathcal{O}(0.1)$, $\\mathcal{O}(0.01)$, etc. For a different dataset, a $\\lambda$ value as small as whats used above could be appropriate - it's hard to tell without an explicit search.\n",
    "\n",
    "Let's now test the opposite limit, with a much larger $\\lambda$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d69beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<00:55,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], CE Loss: 1.5179, DA Loss: 0.2504 Source Acc: 42.14%, Target Acc: 46.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:11<00:46,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], CE Loss: 1.4942, DA Loss: 0.0012 Source Acc: 43.36%, Target Acc: 48.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:17<00:41,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], CE Loss: 1.4846, DA Loss: 0.0003 Source Acc: 43.71%, Target Acc: 49.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:23<00:36,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], CE Loss: 1.4870, DA Loss: 0.0002 Source Acc: 43.71%, Target Acc: 49.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:30<00:30,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], CE Loss: 1.4846, DA Loss: 0.0001 Source Acc: 43.71%, Target Acc: 49.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:36<00:24,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], CE Loss: 1.4898, DA Loss: 0.0001 Source Acc: 43.71%, Target Acc: 49.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:42<00:18,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], CE Loss: 1.4867, DA Loss: 0.0001 Source Acc: 43.30%, Target Acc: 48.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:49<00:12,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], CE Loss: 1.4928, DA Loss: 0.0001 Source Acc: 43.27%, Target Acc: 48.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:57<00:06,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], CE Loss: 1.4808, DA Loss: 0.0001 Source Acc: 43.31%, Target Acc: 48.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:05<00:00,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], CE Loss: 1.4799, DA Loss: 0.0001 Source Acc: 43.74%, Target Acc: 49.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "DA_model = CNN().to(device)  # CNN should return (logits, z)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(DA_model.parameters(), lr=5e-3)\n",
    "lambda_mmd = 1000\n",
    "geomloss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=.1) # GeomLoss with Sinkhorn distance, p=2, blur ~ σ; tune as needed\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# --- Training ---\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    DA_model.train()\n",
    "    running_loss, correct_s, correct_t, total_s, total_t = 0.0, 0, 0, 0, 0\n",
    "    running_DA, running_CE = 0.0, 0.0\n",
    "\n",
    "    for (source_imgs, source_labels), (target_imgs, target_labels) in zip(source_train_loader, target_train_loader):\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs, target_labels = target_imgs.to(device), target_labels.to(device)\n",
    "        source_imgs, target_imgs = source_imgs.float(), target_imgs.float()\n",
    "\n",
    "        # --- Concatenate images and forward ---\n",
    "        combined_imgs = torch.cat([source_imgs, target_imgs], dim=0)\n",
    "        logits, z = DA_model(combined_imgs)\n",
    "\n",
    "        # --- Split latent vectors and outputs ---\n",
    "        z_s, z_t = z[:source_imgs.size(0)], z[source_imgs.size(0):]\n",
    "        logits_s = logits[:source_imgs.size(0)]\n",
    "        logits_t = logits[source_imgs.size(0):]\n",
    "\n",
    "        # --- Compute losses ---\n",
    "        ce_loss = criterion(logits_s, source_labels)\n",
    "        mmd_loss = geomloss_fn(z_s, z_t)  # ← GeomLoss replaces MMD here\n",
    "        total_loss = ce_loss + lambda_mmd * mmd_loss\n",
    "        \n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_model = copy.deepcopy(DA_model.state_dict())\n",
    "\n",
    "\n",
    "        # --- Backpropagation ---\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Metrics ---\n",
    "        running_loss += total_loss.item()\n",
    "        running_CE += ce_loss.item()\n",
    "        running_DA += mmd_loss.item()\n",
    "        _, predicted_s = logits_s.max(1)\n",
    "        total_s += source_labels.size(0)\n",
    "        correct_s += predicted_s.eq(source_labels).sum().item()\n",
    "        _, predicted_t = logits_t.max(1)\n",
    "        total_t += target_labels.size(0)\n",
    "        correct_t += predicted_t.eq(target_labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CE Loss: {running_CE/len(source_train_loader):.4f}, DA Loss: {running_DA/len(source_train_loader):.4f} Source Acc: {100*correct_s/total_s:.2f}%, Target Acc: {100*correct_t/total_t:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b483248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (MNIST): 43.55%\n",
      "Accuracy on target domain (MNIST-M): 50.10%\n"
     ]
    }
   ],
   "source": [
    "# Test on source domain (SDSS)\n",
    "DA_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(DA_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (MNIST): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (DESI)\n",
    "target_accuracy = test_model(DA_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (MNIST-M): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117d60e",
   "metadata": {},
   "source": [
    "This was clearly too much DA...and not great performance either. And in this case, we can see that the target domain performance is better than the source! Clearly the loss function didn't let the model learn to classify galaxies properly in the first place, because the DA was too strong.\n",
    "\n",
    "So how can we be certain we're doing things optimally? What one would usually think to do in this case is some sort of search, trying out different values of $\\lambda$ within an appropriate range and seeing what works best. But also, the results could be sensitive to the choice of seed, so you should marginalize over multiple realizations and make it a grid search. But also, we havent even thought about what to do with $\\sigma$, which is important as well, so maybe a cube search...?\n",
    "\n",
    "I think you see where this is going. There's a lot of things that are important here. Here, I'll talk about a *trainable* way to avoid the $\\lambda$ search, using an idea from Bayesian inference. See [this paper](https://arxiv.org/abs/1705.07115).\n",
    "\n",
    "The overall idea is two introduce trainable coefficients for the two loss terms. Let us introduce two trainable scalar parameters $\\eta_1$ and $\\eta_2$, and define the following loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2 \\eta_1} \\mathcal{L}_\\text{CE} + \\frac{1}{2 \\eta_2} \\mathcal{L}_\\text{DA} + \\log(|\\eta_1 \\eta_2|) \n",
    "$$\n",
    "\n",
    "These values can then be optimized jointly with the model parameters, avoiding the need for a user-specified weighting for the loss terms.\n",
    "\n",
    "Let's think about this loss function before proceeding: \n",
    "\n",
    "* What happens to each loss term as its corresponding $\\eta_i \\to 0$.\n",
    "* What does the third term $\\log(|\\eta_1 \\eta_2|)$ do?\n",
    "* We saw that strongly preffering $\\mathcal{L}_\\text{DA}$ over $\\mathcal{L}_\\text{CE}$ will result in bad performance. This trainable prescription does not know a priori that doing so is bad. Is there any way to enforce this? *Hint*: Think about what range the parameters should be in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e33780c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<01:02,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], CE Loss: 1.0649, DA Loss: 0.7426 Source Acc: 61.80%, Target Acc: 53.52% eta_1: 0.16757038235664368 eta_2: 0.962223470211029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:12<00:51,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], CE Loss: 0.7855, DA Loss: 0.9814 Source Acc: 70.90%, Target Acc: 58.69% eta_1: 0.19984300434589386 eta_2: 0.9710079431533813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:19<00:43,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], CE Loss: 0.7083, DA Loss: 0.8384 Source Acc: 73.79%, Target Acc: 60.76% eta_1: 0.22405940294265747 eta_2: 0.9531427025794983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:25<00:37,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], CE Loss: 0.6522, DA Loss: 0.8060 Source Acc: 75.38%, Target Acc: 61.34% eta_1: 0.24363020062446594 eta_2: 0.9321667551994324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:32<00:32,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], CE Loss: 0.6171, DA Loss: 0.7022 Source Acc: 77.31%, Target Acc: 62.34% eta_1: 0.26035505533218384 eta_2: 0.8960933089256287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:38<00:25,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], CE Loss: 0.5795, DA Loss: 0.6033 Source Acc: 78.65%, Target Acc: 63.12% eta_1: 0.2748684287071228 eta_2: 0.8479979038238525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:44<00:19,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], CE Loss: 0.5495, DA Loss: 0.4910 Source Acc: 80.10%, Target Acc: 63.83% eta_1: 0.2879581153392792 eta_2: 0.7836197018623352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:51<00:12,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], CE Loss: 0.5048, DA Loss: 0.4716 Source Acc: 81.55%, Target Acc: 63.58% eta_1: 0.29913079738616943 eta_2: 0.7349061965942383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:57<00:06,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], CE Loss: 0.4799, DA Loss: 0.4028 Source Acc: 82.85%, Target Acc: 63.76% eta_1: 0.30937638878822327 eta_2: 0.6769939064979553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:02<00:00,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], CE Loss: 0.4461, DA Loss: 0.3634 Source Acc: 84.34%, Target Acc: 64.03% eta_1: 0.31849029660224915 eta_2: 0.6268103122711182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "DA_model = CNN().to(device)  # CNN should return (logits, z)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(DA_model.parameters(), lr=1e-3)\n",
    "geomloss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=10) # GeomLoss with Sinkhorn distance, p=2, blur ~ σ; tune as needed\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "## initialize trainable coefficients\n",
    "eta_1 = nn.Parameter(torch.tensor(0.1, requires_grad=True))  # Coefficient for CE loss\n",
    "eta_2 = nn.Parameter(torch.tensor(1.0, requires_grad=True))  #\n",
    "\n",
    "optimizer.add_param_group({\"params\": [eta_1, eta_2]})\n",
    "\n",
    "# --- Training ---\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    DA_model.train()\n",
    "    running_loss, correct_s, correct_t, total_s, total_t = 0.0, 0, 0, 0, 0\n",
    "    running_DA, running_CE = 0.0, 0.0\n",
    "\n",
    "    for (source_imgs, source_labels), (target_imgs, target_labels) in zip(source_train_loader, target_train_loader):\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs, target_labels = target_imgs.to(device), target_labels.to(device)\n",
    "        source_imgs, target_imgs = source_imgs.float(), target_imgs.float()\n",
    "\n",
    "        # --- Concatenate images and forward ---\n",
    "        combined_imgs = torch.cat([source_imgs, target_imgs], dim=0)\n",
    "        logits, z = DA_model(combined_imgs)\n",
    "\n",
    "        # --- Split latent vectors and outputs ---\n",
    "        z_s, z_t = z[:source_imgs.size(0)], z[source_imgs.size(0):]\n",
    "        logits_s = logits[:source_imgs.size(0)]\n",
    "        logits_t = logits[source_imgs.size(0):]\n",
    "\n",
    "        # --- Compute losses ---\n",
    "        ce_loss = criterion(logits_s, source_labels)\n",
    "        mmd_loss = geomloss_fn(z_s, z_t)  # ← GeomLoss replaces MMD here\n",
    "        total_loss = (2 * eta_1**2)**-1 * ce_loss + (2 * eta_2**2)**-1 * mmd_loss + torch.log(eta_1 * eta_2)  # ← Using trainable coefficients\n",
    "        \n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_model = copy.deepcopy(DA_model.state_dict())\n",
    "\n",
    "        # --- Backpropagation ---\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(DA_model.parameters(), max_norm=10.0)\n",
    "        eta_1.data.clamp_(min=1e-3)\n",
    "        eta_2.data.clamp_(min=0.25*eta_1.data.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Metrics ---\n",
    "        running_loss += total_loss.item()\n",
    "        running_CE += ce_loss.item()\n",
    "        running_DA += mmd_loss.item()\n",
    "        _, predicted_s = logits_s.max(1)\n",
    "        total_s += source_labels.size(0)\n",
    "        correct_s += predicted_s.eq(source_labels).sum().item()\n",
    "        _, predicted_t = logits_t.max(1)\n",
    "        total_t += target_labels.size(0)\n",
    "        correct_t += predicted_t.eq(target_labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CE Loss: {running_CE/len(source_train_loader):.4f}, DA Loss: {running_DA/len(source_train_loader):.4f} Source Acc: {100*correct_s/total_s:.2f}%, Target Acc: {100*correct_t/total_t:.2f}%\", \"eta_1:\", eta_1.item(), \"eta_2:\", eta_2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47051e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (MNIST): 74.55%\n",
      "Accuracy on target domain (MNIST-M): 66.05%\n"
     ]
    }
   ],
   "source": [
    "# Test on source domain (SDSS)\n",
    "DA_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(DA_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (MNIST): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (DESI)\n",
    "target_accuracy = test_model(DA_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (MNIST-M): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d26b64",
   "metadata": {},
   "source": [
    "In this case you've seen that we've outperformed no-DA and both fixed-coefficient DA examples by ~5%, without any grid search whatsoever! This trainable prescription largely avoided the need to tune these hyperparameters. Keep in mind we did use some inductive bias: we initialized the $\\eta_i$ differently (think as to why we did this), and clipped them in some reasonable way. This was simply to give the parameters a nudge in the right direction.\n",
    "\n",
    "The last thing we want to study is our treatment of $\\sigma$. In the last experiments, we used a fixed value of $\\sigma = 10$, but now lets think about the assumption that a single value of $\\sigma$ is optimal. It may not be.\n",
    "\n",
    "Keep in mind that as $\\sigma \\to 0$ the Sinkhorn plan interpolates closer to $\\text{OT}_0$, and $\\sigma \\to \\infty$ interpolates closer to MMD. Further, MMD is cheaper to compute than $\\text{OT}_0$ (fewer Sinkhorn iterations).\n",
    "\n",
    "Let's now try a simple annealing scheme, so that $\\sigma$ is larger at the beginning of training, when the latent spaces are very misaligned, and smaller at the end, when we want to really focus on domain alignment.\n",
    "\n",
    "Consider the $\\sigma$ scheduler given by:\n",
    "\n",
    "$$\n",
    "\\sigma = 0.05 * 0.95^\\ell \\quad \\text{where} \\; \\ell \\; \\text{is epoch number}\n",
    "$$\n",
    "\n",
    "We will test this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63ed88c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<00:55,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], CE Loss: 1.0962, DA Loss: 0.6699 Source Acc: 61.09%, Target Acc: 51.83% eta_1: 0.16784130036830902 eta_2: 0.953438401222229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:12<00:49,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], CE Loss: 0.7716, DA Loss: 0.8929 Source Acc: 71.50%, Target Acc: 56.96% eta_1: 0.1990041434764862 eta_2: 0.9502055048942566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:18<00:44,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], CE Loss: 0.7016, DA Loss: 0.8773 Source Acc: 73.80%, Target Acc: 58.26% eta_1: 0.22279058396816254 eta_2: 0.942546546459198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:25<00:38,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], CE Loss: 0.6541, DA Loss: 0.7759 Source Acc: 75.46%, Target Acc: 59.29% eta_1: 0.2422061413526535 eta_2: 0.918634295463562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:31<00:31,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], CE Loss: 0.6058, DA Loss: 0.6485 Source Acc: 77.34%, Target Acc: 60.80% eta_1: 0.2584995627403259 eta_2: 0.8739157915115356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:38<00:25,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], CE Loss: 0.5537, DA Loss: 0.6054 Source Acc: 79.10%, Target Acc: 61.16% eta_1: 0.2724589407444 eta_2: 0.8333373069763184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:45<00:20,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], CE Loss: 0.5264, DA Loss: 0.5404 Source Acc: 80.96%, Target Acc: 61.67% eta_1: 0.28491702675819397 eta_2: 0.7866923809051514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:52<00:13,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], CE Loss: 0.4876, DA Loss: 0.4750 Source Acc: 82.54%, Target Acc: 62.75% eta_1: 0.29583096504211426 eta_2: 0.7372545599937439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:57<00:06,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], CE Loss: 0.4490, DA Loss: 0.3963 Source Acc: 84.35%, Target Acc: 62.26% eta_1: 0.3053573668003082 eta_2: 0.6750503778457642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:03<00:00,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], CE Loss: 0.4207, DA Loss: 0.3331 Source Acc: 85.59%, Target Acc: 62.83% eta_1: 0.3140632212162018 eta_2: 0.6141291260719299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "DA_model = CNN().to(device)  # CNN should return (logits, z)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(DA_model.parameters(), lr=1e-3)\n",
    "geomloss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=10) # GeomLoss with Sinkhorn distance, p=2, blur ~ σ; tune as needed\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "## initialize trainable coefficients\n",
    "eta_1 = nn.Parameter(torch.tensor(0.1, requires_grad=True))  # Coefficient for CE loss\n",
    "eta_2 = nn.Parameter(torch.tensor(1.0, requires_grad=True))  #\n",
    "\n",
    "optimizer.add_param_group({\"params\": [eta_1, eta_2]})\n",
    "\n",
    "# --- Training ---\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    DA_model.train()\n",
    "    running_loss, correct_s, correct_t, total_s, total_t = 0.0, 0, 0, 0, 0\n",
    "    running_DA, running_CE = 0.0, 0.0\n",
    "\n",
    "    for (source_imgs, source_labels), (target_imgs, target_labels) in zip(source_train_loader, target_train_loader):\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs, target_labels = target_imgs.to(device), target_labels.to(device)\n",
    "        source_imgs, target_imgs = source_imgs.float(), target_imgs.float()\n",
    "\n",
    "        # --- Concatenate images and forward ---\n",
    "        combined_imgs = torch.cat([source_imgs, target_imgs], dim=0)\n",
    "        logits, z = DA_model(combined_imgs)\n",
    "\n",
    "        # --- Split latent vectors and outputs ---\n",
    "        z_s, z_t = z[:source_imgs.size(0)], z[source_imgs.size(0):]\n",
    "        logits_s = logits[:source_imgs.size(0)]\n",
    "        logits_t = logits[source_imgs.size(0):]\n",
    "\n",
    "        # --- Compute losses ---\n",
    "        ce_loss = criterion(logits_s, source_labels)\n",
    "        dynamic_blur = 10 * 0.95**epoch\n",
    "        mmd_loss = geomloss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=dynamic_blur)(z_s, z_t)  # ← GeomLoss replaces MMD here\n",
    "        total_loss = (2 * eta_1**2)**-1 * ce_loss + (2 * eta_2**2)**-1 * mmd_loss + torch.log(eta_1 * eta_2)  # ← Using trainable coefficients\n",
    "        \n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_model = copy.deepcopy(DA_model.state_dict())\n",
    "\n",
    "        # --- Backpropagation ---\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(DA_model.parameters(), max_norm=10.0)\n",
    "        eta_1.data.clamp_(min=1e-3)\n",
    "        eta_2.data.clamp_(min=0.25*eta_1.data.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Metrics ---\n",
    "        running_loss += total_loss.item()\n",
    "        running_CE += ce_loss.item()\n",
    "        running_DA += mmd_loss.item()\n",
    "        _, predicted_s = logits_s.max(1)\n",
    "        total_s += source_labels.size(0)\n",
    "        correct_s += predicted_s.eq(source_labels).sum().item()\n",
    "        _, predicted_t = logits_t.max(1)\n",
    "        total_t += target_labels.size(0)\n",
    "        correct_t += predicted_t.eq(target_labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CE Loss: {running_CE/len(source_train_loader):.4f}, DA Loss: {running_DA/len(source_train_loader):.4f} Source Acc: {100*correct_s/total_s:.2f}%, Target Acc: {100*correct_t/total_t:.2f}%\", \"eta_1:\", eta_1.item(), \"eta_2:\", eta_2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27621377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (MNIST): 73.30%\n",
      "Accuracy on target domain (MNIST-M): 66.25%\n"
     ]
    }
   ],
   "source": [
    "# Test on source domain (SDSS)\n",
    "DA_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(DA_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (MNIST): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (DESI)\n",
    "target_accuracy = test_model(DA_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (MNIST-M): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a14c1",
   "metadata": {},
   "source": [
    "Ok, we see in this case the scheduler for $\\sigma$ does not change things that much. Thats ok! We have shown in [this paper](https://arxiv.org/abs/2501.14048) that a $\\sigma$ scheduler based on the 2-norm between latent spaces not only results in better performance on both domains, but is also significantly faster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
