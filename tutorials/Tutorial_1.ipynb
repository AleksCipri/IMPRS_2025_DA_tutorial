{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnyG_OIBbJqo"
      },
      "source": [
        "Practice with Distance-based and Adversarial Domain Adaptation Methods\n",
        "---\n",
        "##### Author: [Sneh Pandya](https://snehjp2.github.io)\n",
        "\n",
        "You've seen in the lectures how a variety of domain adaptation (DA) techniques can improve the generalization capabilities of neural networks (NNs), even when labeled data is only available in one domain. In this tutorial, you'll get hands-on experience applying DA to address a covariate shift scenario—where the input data distribution changes between training and deployment domains, but the underlying classification task remains the same.\n",
        "\n",
        "\n",
        "<img src=\"https://snehjp2.github.io/images/confused.png\" alt=\"confused\" width=\"600\"/>\n",
        "\n",
        "We'll demonstrate this with a simple neural network trained on the [MNIST dataset](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) (source domain) and evaluated on the [MNIST-M dataset](https://www.kaggle.com/datasets/aquibiqbal/mnistm) (target domain). While both datasets have the same label space (10 classes), the pixel distributions differ significantly, providing a natural testbed for domain adaptation techniques. In the interest of getting the point across simply, we will focus on a subset of both datasets, classifying 3 of the 10 classes (digits 1, 4, and 8). Our tutorial will use PyTorch, starting with the following imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEqq_GzmbsX4"
      },
      "outputs": [],
      "source": [
        "# torch related inputs\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST, ImageFolder\n",
        "import copy\n",
        "\n",
        "# miscellaneous\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import urllib\n",
        "\n",
        "# geomloss provides distance measure that are torch/CUDA compatible. \n",
        "# Uncomment line below to install it, if you don't have it installed.\n",
        "\n",
        "# !pip install -q geomloss\n",
        "\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "def set_all_seeds(seed=42):\n",
        "\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # For Python's hash seed\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Optional: enforce deterministic algorithms where possible\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "set_all_seeds()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNIOC_s8ipAH"
      },
      "source": [
        "Problem Setup\n",
        "---\n",
        "\n",
        "- **Neural Network**:  \n",
        "Let $f_\\theta$ denote a neural network classifier with parameters $\\theta$, which maps an input image $x \\in \\mathbb{R}^n$ to predicted class probabilities $\\hat{y} = f_\\theta(x) \\in \\mathbb{R}^K$. The network is decomposed into a feature extractor $\\phi_\\theta : \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ and a classifier head $g_\\theta : \\mathbb{R}^d \\rightarrow \\mathbb{R}^K$, such that $f_\\theta(x) = \\text{softmax}(g_\\theta(\\phi_\\theta(x)))$.  \n",
        "We define the **latent representation** $z = \\phi_\\theta(x)$ as the output of the final hidden layer (before the logits). This representation (also called the *latent vector* or *latent space*) will be used for domain alignment.\n",
        "\n",
        "- **Source domain dataset**:  \n",
        "$\\mathcal{D}_s = \\{(x_s^{(i)}, y_s^{(i)})\\}_{i=1}^{N_s}$,  \n",
        "where $x_s^{(i)} \\sim p_s(x)$ are grayscale digit images from MNIST, and $y_s^{(i)} \\in \\{1, 4, 8 \\}$ are the corresponding digit labels.\n",
        "\n",
        "- **Target domain dataset**:  \n",
        "$\\mathcal{D}_t = \\{x_t^{(j)}\\}_{j=1}^{N_t}$,  \n",
        "where $x_t^{(j)} \\sim p_t(x)$ are unlabeled images from MNIST-M.  \n",
        "We assume the label distributions are aligned, i.e., $p_s(y|x) = p_t(y|x)$, but the input distributions differ: $p_s(x) \\neq p_t(x)$.  \n",
        "Our goal is to adapt $f_\\theta$ using **only labeled source data** and **unlabeled target data**, so that it performs well on the target domain.\n",
        "\n",
        "- **Classification loss**:  \n",
        "On the source domain, we minimize the supervised cross-entropy loss between predicted and true labels:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{CE}}(\\theta) = -\\frac{1}{N_s} \\sum_{i=1}^{N_s} \\log f_\\theta^{(y_s^{(i)})}(x_s^{(i)}),\n",
        "$$\n",
        "\n",
        "where $f_\\theta^{(k)}(x)$ denotes the predicted probability for class $k$, and $K = 3$ is the number of classes.\n",
        "\n",
        "---\n",
        "Let's start by defining our necessary ingredients, starting with the data. MNIST ($\\mathcal{D}_s$) is readily available from `torchvision`, but we'll have to download MNIST-M ($\\mathcal{D}_t$) from source. MNIST is single channel and MNISTM is three channel; we'll copy the single channel image across three channels so that the images can be loaded into the model properly. We will limit ourselves to a subset for the data so that you can train things rather quickly. \n",
        "\n",
        "Recall that images are typically normalized to be in the range [-1,1] for gradient stability during training. This is done via [z-score normalization](https://en.wikipedia.org/wiki/Standard_score). I've precalculated the means and standard deviations of the datasets for you. Feel free to check this yourself! This is an often not-discussed/overlooked aspect of training NNs, but data normalization is **immensely important**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT8PkB7aNFwm"
      },
      "outputs": [],
      "source": [
        "mnist_mean = (0.1307,) * 3 ## found online\n",
        "mnist_std = (0.3015,) * 3 ## found online\n",
        "\n",
        "mnistm_mean = (0.4579, 0.4621, 0.4082) #I've precalculated these\n",
        "mnistm_std = (0.1880, 0.1755, 0.1956) #I've precalculated these"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urvqDxNen4x8"
      },
      "outputs": [],
      "source": [
        "# Define target digits\n",
        "target_digits = [1, 4, 8]\n",
        "\n",
        "# Define transform\n",
        "mnist_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mnist_mean, mnist_std)\n",
        "])\n",
        "\n",
        "# Load full MNIST datasets\n",
        "mnist_train_full = MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n",
        "mnist_test_full = MNIST(root='./data', train=False, download=True, transform=mnist_transform)\n",
        "\n",
        "# Filter indices for only digits 1, 4, 8\n",
        "def filter_indices(dataset, allowed_labels):\n",
        "    return [i for i, (_, label) in enumerate(dataset) if label in allowed_labels]\n",
        "\n",
        "filtered_train_indices = filter_indices(mnist_train_full, target_digits)\n",
        "filtered_test_indices = filter_indices(mnist_test_full, target_digits)\n",
        "\n",
        "# Create subsets for training and testing\n",
        "train_subset_size = 5000\n",
        "test_subset_size = 1000\n",
        "\n",
        "train_indices = torch.tensor(filtered_train_indices)[torch.randperm(len(filtered_train_indices))[:train_subset_size]]\n",
        "test_indices = torch.tensor(filtered_test_indices)[torch.randperm(len(filtered_test_indices))[:test_subset_size]]\n",
        "\n",
        "mnist_train = Subset(mnist_train_full, train_indices)\n",
        "mnist_test = Subset(mnist_test_full, test_indices)\n",
        "\n",
        "# Define DataLoaders\n",
        "source_train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
        "source_test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-l2xzAPomm_",
        "outputId": "8d146d0d-d32f-4569-b73a-a20d0bf1f786"
      },
      "outputs": [],
      "source": [
        "## Downloading MNIST-M and defining data loaders. MNIST-M labels are included but will not be used during training.\n",
        "\n",
        "url = \"https://github.com/mashaan14/MNIST-M/raw/main/MNIST-M.zip\"\n",
        "zip_path = \"./MNIST-M.zip\"\n",
        "extract_path = \"./data/mnist_m\"\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading MNIST‑M dataset...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    print(\"Extracting MNIST‑M...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Files extracted to\", extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ics8lffWqKsb"
      },
      "outputs": [],
      "source": [
        "# Transform for MNIST-M\n",
        "mnistm_transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mnistm_mean, mnistm_std)\n",
        "])\n",
        "\n",
        "# Load full MNIST-M dataset\n",
        "mnistm_train_full = ImageFolder(root=os.path.join(extract_path, \"MNIST-M/training\"), transform=mnistm_transform)\n",
        "mnistm_test_full = ImageFolder(root=os.path.join(extract_path, \"MNIST-M/testing\"), transform=mnistm_transform)\n",
        "\n",
        "# Filter indices to only keep labels 1, 4, 8\n",
        "def filter_indices(dataset, allowed_labels):\n",
        "    return [i for i, (_, label) in enumerate(dataset) if label in allowed_labels]\n",
        "\n",
        "filtered_train_indices = filter_indices(mnistm_train_full, target_digits)\n",
        "filtered_test_indices = filter_indices(mnistm_test_full, target_digits)\n",
        "\n",
        "# Create subsets for training and testing\n",
        "train_indices = torch.tensor(filtered_train_indices)[torch.randperm(len(filtered_train_indices))[:train_subset_size]]\n",
        "test_indices = torch.tensor(filtered_test_indices)[torch.randperm(len(filtered_test_indices))[:test_subset_size]]\n",
        "\n",
        "# Create subset datasets\n",
        "mnistm_train = Subset(mnistm_train_full, train_indices)\n",
        "mnistm_test = Subset(mnistm_test_full, test_indices)\n",
        "\n",
        "# Define DataLoaders\n",
        "target_train_loader = DataLoader(mnistm_train, batch_size=64, shuffle=True)\n",
        "target_test_loader = DataLoader(mnistm_test, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVhsIryxzVGB"
      },
      "source": [
        "Now that the data is downloaded, lets just sanity check that the images look like what we would expect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect one example per digit\n",
        "def get_examples_by_label(dataset, target_digits):\n",
        "    label_to_img = {}\n",
        "    for img, label in dataset:\n",
        "\n",
        "        label = int(label)\n",
        "        if label in target_digits and label not in label_to_img:\n",
        "            label_to_img[label] = img\n",
        "        if len(label_to_img) == len(target_digits):\n",
        "            break\n",
        "    return label_to_img\n",
        "\n",
        "def unnormalize(img, mean, std):\n",
        "    img = img.clone()\n",
        "    for c in range(img.shape[0]):\n",
        "        img[c] = img[c] * std[c] + mean[c]\n",
        "    return img\n",
        "\n",
        "# Get one image per digit\n",
        "mnist_digits = get_examples_by_label(mnist_train, target_digits)\n",
        "mnistm_digits = get_examples_by_label(mnistm_train, target_digits)\n",
        "\n",
        "# Plot the images\n",
        "fig, axes = plt.subplots(2, len(target_digits), figsize=(12, 6))\n",
        "\n",
        "for idx, digit in enumerate(target_digits):\n",
        "    img_mnist = unnormalize(mnist_digits[digit], mnist_mean, mnist_std)\n",
        "    axes[0, idx].imshow(img_mnist.permute(1, 2, 0).numpy())\n",
        "    axes[0, idx].set_title(f\"Digit {digit}\", fontsize=10)\n",
        "    axes[0, idx].axis(\"off\")\n",
        "\n",
        "    img_mnistm = unnormalize(mnistm_digits[digit], mnistm_mean, mnistm_std)\n",
        "    axes[1, idx].imshow(img_mnistm.permute(1, 2, 0).numpy())\n",
        "    axes[1, idx].set_title(f\"Digit {digit}\", fontsize=10)\n",
        "    axes[1, idx].axis(\"off\")\n",
        "\n",
        "axes[0, 0].set_ylabel(\"MNIST\", fontsize=12)\n",
        "axes[1, 0].set_ylabel(\"MNIST-M\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Images look good, now lets just check that the shapes are the same for training, and that the z-score normalization statistics look ok."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEvBrjMhq1fh",
        "outputId": "0df7a2e2-4546-4bd8-fcd4-3ba134452d91"
      },
      "outputs": [],
      "source": [
        "# Get a sample image from each dataset\n",
        "mnist_image = next(iter(DataLoader(mnist_train, batch_size=1)))[0]\n",
        "mnistm_image = next(iter(DataLoader(mnistm_train, batch_size=1)))[0]\n",
        "\n",
        "## TO DO ##\n",
        "# Print shapes and statistics of the images\n",
        "# Before printing, what do you expect the shapes and statistics to be?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPIG_AjAz4tC"
      },
      "source": [
        "Now let's define our model. We will write a simple CNN thats designed for classification, so that things can run quickly. There are a couple things to think carefully about here:\n",
        "\n",
        "- The important difference with models used for DA training is that you are using the latent vector $z$ and the output logits. Write the forward pass to reflect this. That is, the full NN $f_\\theta$ should output both the $\\phi_\\theta$ branch and $g_\\theta$ branch.\n",
        "- The efficacy of DA is primarily dependent on the expressivity of $\\phi_\\theta$. Do you expect this model to work well? Why or why not?\n",
        "- Do we want the dimension of the latent vector to be small or large? Why or why not? \n",
        "- There are other NN architectural components (not included) that generally are thought to help generalization. What are some of these?\n",
        "\n",
        "*Stretch Question:* You may have heard about symmetry-aware [equivariant NNs](https://arxiv.org/pdf/1602.07576). A NN like the one defined below is manifestly translation-equivariant, but many images in Nature contain higher-order symmetries (e.g., rotations, reflections). Would you expect an equivariant model to generalize better? Under what conditions would this happen, and under what conditions would this not? Hint: Think about how the conditional distribution of the labels transforms under symmetries.\n",
        "\n",
        "Try to make a guess for how our model will perform before you proceed. Make note of your answers below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgDxeCNkrfaP"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2), \n",
        "        )\n",
        "\n",
        "        self.flatten_dim = 128 * 7 * 7\n",
        "\n",
        "\n",
        "        # Fully connected classifier\n",
        "        ## TO DO ##\n",
        "        # Write the linear layers to have the structure: Linear(128 * 7 * 7, 256) -> ReLU -> Dropout(0.5) -> Linear(256, 128) -> ReLU -> Linear(128, num_classes)\n",
        "        self.classifier = None\n",
        "        \n",
        "    ## TO DO ##\n",
        "    # Write the forward pass. Make sure to flatten the output of the convolutional layers before passing it to the classifier, and to return the latent z.\n",
        "    def forward(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets proceed to train this model. We will train the CNN without domain adaptation on $\\mathcal{D}_s$, and test it on both $\\mathcal{D}_s$ and $\\mathcal{D}_t$. This will serve as a performance baseline. Much of this training loop is standard. We will use `AdamW` optimizer with a learning rate of `1e-3`. Usually, one would split their training set further to include a validation set, and the best model will be saved according to that. For this simple example, we will just save the model according to the training set performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zcWuV7Iz9G8",
        "outputId": "d15a7ac8-f866-4fed-ecfc-03ecd6617660"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "CE_only_model = CNN().to(device)\n",
        "label_map = {1: 0, 4: 1, 8: 2}\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(CE_only_model.parameters(), lr=1e-3)\n",
        "best_model = None\n",
        "best_loss = float('inf')\n",
        "num_epochs = 3\n",
        "\n",
        "# Training loop\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    CE_only_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in source_train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        labels = torch.tensor([label_map[int(l)] for l in labels], device=device)\n",
        "\n",
        "        ## TO DO ##\n",
        "        # Forward pass through the model and compute loss. This is just two lines!\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if loss.item() < best_loss:\n",
        "            best_loss = loss.item()\n",
        "            best_model = copy.deepcopy(CE_only_model.state_dict())\n",
        "\n",
        "\n",
        "        # Track training stats\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]  Loss: {running_loss/len(source_train_loader):.4f}  Accuracy: {100*correct/total:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-N6cHVuAAzc"
      },
      "source": [
        "The loss went down and accuracy went up. All is well. For a more complex dataset, this would probably be trained for longer, and with some additional bells and whistles during training (validation, L2-regularization, gradient clipping, etc.) for better performance. But this will suffice for now. Now, lets test the model on both domains!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-14EeC_-d_V",
        "outputId": "3eb075a6-31d1-4ae1-a2ed-f449bd8fcb9f"
      },
      "outputs": [],
      "source": [
        "def test_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = torch.tensor([label_map[int(l)] for l in labels], device=device)\n",
        "\n",
        "            outputs, _ = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on source domain (MNIST)\n",
        "CE_only_model.load_state_dict(best_model)\n",
        "source_accuracy = test_model(CE_only_model, source_test_loader, device)\n",
        "print(f\"Accuracy on source domain (MNIST): {source_accuracy:.2f}%\")\n",
        "\n",
        "# Test on target domain (MNIST-M)\n",
        "target_accuracy = test_model(CE_only_model, target_test_loader, device)\n",
        "print(f\"Accuracy on target domain (MNIST-M): {target_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okwwlJkhBO3G"
      },
      "source": [
        "We see that the model has a ~40% discrepancy between the source and target domain. Despite this pretty mild covariate shift, the model is failing horribly to generalize! Moreover, we saw from visualizing the images earlier that MNIST and MNIST-M dont look *too* different. This is important. **Typically trained NNs aren't learning dataset-invariant features**. Now, lets move to domain adaptation and see if we can shrink this performance gap between $\\mathcal{D}_s$ and $\\mathcal{D}_t$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbiOUEio8twD"
      },
      "source": [
        "## Training with Distance-based Domain Adaptation\n",
        "\n",
        "We use [`geomloss`](https://www.kernel-operations.io/geomloss/api/pytorch-api.html) to compute distributional distances between the latent representations of source and target examples. This library provides differentiable loss functions between samples or measures, making it well-suited for domain adaptation (DA). In this setup, the distance serves to **align the source and target latent distributions** without using target labels during backpropagation.\n",
        "\n",
        "There are a variety of distances implemented in `geomloss`, such as Sinkhorn divergences and energy distances. In this example, we use the **energy distance**, which is equivalent to a Gaussian Maximum Mean Discrepancy (MMD) with a fixed bandwidth. You are encouraged to experiment with [other options](https://www.kernel-operations.io/geomloss/api/pytorch-api.html) and tune the DA loss strength $\\lambda_{\\text{MMD}}$ to explore their effects on adaptation performance.\n",
        "\n",
        "We'll train our model for longer than before. Also, since we have access to our target labels, we’ll track target domain accuracy during training. In a general problem setting where target labels are unavailable, this wouldn't be possible — but for teaching purposes, it's a useful diagnostic. Importantly, **target labels are not used for backpropagation** and do not influence the model updates. We also save the model only according to training loss performance, not target domain accuracy.\n",
        "\n",
        "\n",
        "#### Gaussian Maximum Mean Discrepancy (MMD)\n",
        "\n",
        "Let $z_s \\sim \\mathbb{P}_s$ and $z_t \\sim \\mathbb{P}_t$ be the latent representations of source and target inputs respectively. The squared Gaussian MMD between these two distributions is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}_{\\text{MMD}}(\\theta) &\\equiv \\text{MMD}^2(\\mathbb{P}_s, \\mathbb{P}_t) \\\\\n",
        "&= \\mathbb{E}_{z, z' \\sim \\mathbb{P}_s} [k(z, z')] + \\mathbb{E}_{\\tilde{z}, \\tilde{z}' \\sim \\mathbb{P}_t} [k(\\tilde{z}, \\tilde{z}')] - 2 \\mathbb{E}_{z \\sim \\mathbb{P}_s,\\, \\tilde{z} \\sim \\mathbb{P}_t} [k(z, \\tilde{z})],\n",
        "\\end{align}\n",
        "\n",
        "where $k(z, \\tilde{z}) = \\exp\\left( -\\frac{\\|z - \\tilde{z}\\|^2}{2\\sigma^2} \\right)$ is a Gaussian kernel with bandwidth $\\sigma$. The full loss minimized during training is:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{total}}(\\theta) = \\mathcal{L}_{\\text{CE}}(\\theta) + \\lambda_{\\text{MMD}} \\cdot \\mathcal{L}_{\\text{MMD}}(\\theta),\n",
        "$$\n",
        "\n",
        "where $\\lambda_{\\text{MMD}}$ controls the strength of the domain adaptation penalty. The goal is to correctly classify source examples while encouraging the latent feature distributions of source and target data to align.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtiVNws22l0N",
        "outputId": "6b4956ba-6c97-4da3-d74a-c047b66fca7f"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "DA_model = CNN().to(device)\n",
        "\n",
        "# Setup\n",
        "num_epochs = 6\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(DA_model.parameters(), lr=1e-3)\n",
        "lambda_mmd = 0.1\n",
        "geomloss_fn = SamplesLoss(\"energy\", p=2) # GeomLoss with Sinkhorn distance, p=2, blur ~ σ; tune as needed\n",
        "best_model = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Training loop\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    DA_model.train()\n",
        "    running_loss, correct_s, correct_t, total_s, total_t = 0.0, 0, 0, 0, 0\n",
        "    running_DA, running_CE = 0.0, 0.0\n",
        "\n",
        "    # Iterate over source and target data loaders\n",
        "    for (source_imgs, source_labels), (target_imgs, target_labels) in zip(source_train_loader, target_train_loader):\n",
        "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
        "        target_imgs, target_labels = target_imgs.to(device), target_labels.to(device)\n",
        "        source_labels= torch.tensor([label_map[int(l)] for l in source_labels], device=device)\n",
        "        target_labels = torch.tensor([label_map[int(l)] for l in target_labels], device=device)\n",
        "\n",
        "\n",
        "        ## TO DO ##\n",
        "        # Concatenate source and target images, forward through the model. Call `logits` the output of the model and `z` the latent representation.\n",
        "\n",
        "        ## TO DO ##\n",
        "        # Split logits and latent representations between the source and target domains. Call the source latent representation `z_s` and target latent representation `z_t`. Call the logits for source domain `logits_s` and for target domain `logits_t`.\n",
        "\n",
        "        ## TO DO ##\n",
        "        # Compute the losses. Use `criterion` for the cross-entropy loss and `geomloss_fn` for the GeomLoss. Combine them into `total_loss` with the MMD loss scaled by `lambda_mmd`.\n",
        "        \n",
        "        # Model saving\n",
        "        if total_loss.item() < best_loss:\n",
        "            best_loss = total_loss.item()\n",
        "            best_model = copy.deepcopy(DA_model.state_dict())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track training stats\n",
        "        running_loss += total_loss.item()\n",
        "        running_CE += ce_loss.item()\n",
        "        running_DA += mmd_loss.item()\n",
        "        _, predicted_s = logits_s.max(1)\n",
        "        total_s += source_labels.size(0)\n",
        "        correct_s += predicted_s.eq(source_labels).sum().item()\n",
        "        _, predicted_t = logits_t.max(1)\n",
        "        total_t += target_labels.size(0)\n",
        "        correct_t += predicted_t.eq(target_labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], CE Loss: {running_CE/len(source_train_loader):.4f}, DA Loss: {running_DA/len(source_train_loader):.4f} Source Acc: {100*correct_s/total_s:.2f}%, Target Acc: {100*correct_t/total_t:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp9aQbYfB7Y2",
        "outputId": "5685973c-be93-45ec-805b-be5f937a2173"
      },
      "outputs": [],
      "source": [
        "DA_model.load_state_dict(best_model)\n",
        "# Test on source domain (MNIST)\n",
        "source_accuracy = test_model(DA_model, source_test_loader, device)\n",
        "print(f\"Accuracy on source domain (MNIST): {source_accuracy:.2f}%\")\n",
        "\n",
        "# Test on target domain (MNIST-M)\n",
        "target_accuracy = test_model(DA_model, target_test_loader, device)\n",
        "print(f\"Accuracy on target domain (MNIST-M): {target_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6Ko_TpM8qQ4"
      },
      "source": [
        "DA works! This is a a pretty modest ~10% improvement over the basic NN for the target domain, and we also see similar performance on the source domain. This may not seem like a lot, and this is actually expected. Can you think why? Recall that the efficacy of DA relies on the *expressivity* of $\\phi_\\theta$: how well it could learn features. In a CNN, much of the feauture learning is done by convolutional layers; in our architecture we have four! The most expressive CNNs on the block, e.g. ResNetX, have $X \\in \\{18, 34, 50 \\}$ convolutional layers with residual connections to help model convergence. In addition to this, there's several other axes that contribute to how well the model gets aligned:\n",
        "\n",
        "- training configurations\n",
        "- choice of distance kernel\n",
        "- dataset normalization\n",
        "\n",
        "Feel free to experiment on some or all of these axes, especially in making the CNN deeper and more expressive, and you should see the domain alignment increase.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp9O5azp-RJ5"
      },
      "source": [
        "Trying an Adversarial Approach\n",
        "---\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/349424256/figure/fig2/AS:992763986333698@1613704787924/llustration-of-Domain-Adversarial-Neural-Network-DANN-framework-It-is-a-classic-and.png\" alt=\"confused\" width=\"800\"/>\n",
        "\n",
        "\n",
        "We've seen how to implement a distance-based DA approach; now, we turn to another family of domain adaptation (DA) methods: **adversarial approaches**.\n",
        "\n",
        "In adversarial domain adaptation, we again rely on the feature extractor $\\phi_{\\theta_f}(x)$ to map both source and target images into a shared latent space $z = \\phi_{\\theta_f}(x) \\in \\mathbb{R}^d$. The goal is to learn representations that are discriminative for the source task yet domain-invariant, such that a classifier trained on the source domain generalizes to the target.\n",
        "\n",
        "To achieve this, we introduce an auxiliary domain classifier $d_{\\theta_d} : \\mathbb{R}^d \\rightarrow [0,1]$ that tries to distinguish whether a latent vector $z$ comes from the source or target domain. Here, we make explicit that the parameters of the domain classifier are separate, and what are optimized in total is $\\theta = \\{\\theta_f, \\theta_d\\}$.\n",
        "\n",
        "Let $\\mathcal{D}_s = \\{(x_s^{(i)}, y_s^{(i)})\\}_{i=1}^{N_s}$ and $\\mathcal{D}_t = \\{x_t^{(j)}\\}_{j=1}^{N_t}$ as before. During training, we optimize two competing objectives:\n",
        "\n",
        "- **Source classification loss** (same as before):\n",
        "  $$\n",
        "  \\mathcal{L}_{\\text{CE}}(\\theta) = -\\frac{1}{N_s} \\sum_{i=1}^{N_s} \\log f_\\theta^{(y_s^{(i)})}(x_s^{(i)}),\n",
        "  $$\n",
        "\n",
        "- **Domain adversarial loss**:\n",
        "  $$\n",
        "  \\mathcal{L}_{\\text{dom}}(\\theta) = -\\frac{1}{N_s} \\sum_{i=1}^{N_s} \\log d_{\\theta_d}(\\phi_{\\theta_f}(x_s^{(i)}))\n",
        "  - \\frac{1}{N_t} \\sum_{j=1}^{N_t} \\log \\left(1 - d_{\\theta_d}(\\phi_{\\theta_f}(x_t^{(j)}))\\right)\n",
        "  $$\n",
        "\n",
        "The domain classifier $d_{\\theta_d}$ is trained to minimize $\\mathcal{L}_{\\text{dom}}$, correctly distinguishing source from target domains. In contrast, the feature extractor $\\phi_{\\theta_f}$ is trained to maximize this loss (via a gradient reversal layer), thereby encouraging domain-invariant features. Simultaneously, the source classifier is trained to minimize $\\mathcal{L}_{\\text{CE}}$ and distinguish between the classes. This leads to domain-invariant features, because the model must be good at classifying the source domain but must fail at distinguishing features between domains.\n",
        "\n",
        "This adversarial interaction is implemented using a gradient reversal layer (GRL), which multiplies the gradient from $\\mathcal{L}_{\\text{dom}}$ by $-1$ before backpropagating through $\\phi_{\\theta_f}$.\n",
        "\n",
        "The total loss function is:\n",
        "$$\n",
        "\\mathcal{L}_{\\text{total}}(\\theta) = \\mathcal{L}_{\\text{CE}}(\\theta) - \\lambda \\cdot \\mathcal{L}_{\\text{dom}}(\\theta),\n",
        "$$\n",
        "where $\\lambda$ balances the classification and domain alignment objectives.\n",
        "\n",
        "In the next section, we'll implement this framework using the Domain-Adversarial Neural Network (DANN) and compare it directly against MMD-based adaptation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OyH8rGHCSFr"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "# Gradient Reversal Layer (GRL) implementation\n",
        "class GradientReversal(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambda_grl):\n",
        "        ctx.lambda_grl = lambda_grl\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return -ctx.lambda_grl * grad_output, None\n",
        "\n",
        "def grad_reverse(x, lambda_grl=1.0):\n",
        "    return GradientReversal.apply(x, lambda_grl)\n",
        "\n",
        "# Domain Classifier implementation\n",
        "class DomainClassifier(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)  # Binary output\n",
        "        )\n",
        "        self.project = nn.Linear(6272, 128)\n",
        "\n",
        "    def forward(self, z, lambda_grl=1.0):\n",
        "        z_rev = grad_reverse(z, lambda_grl)\n",
        "        z_proj = self.project(z_rev)\n",
        "        return self.net(z_proj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw3X1kAIB9B_",
        "outputId": "05e14653-b70a-41bb-96f2-e1aac0827338"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "adversarial_model = CNN().to(device)\n",
        "domain_classifier = DomainClassifier(in_dim=128).to(device)  # set z_dim to latent dimension of CNN\n",
        "optimizer_DANN = optim.AdamW(\n",
        "    list(adversarial_model.parameters()) + list(domain_classifier.parameters()), lr=1e-3\n",
        ")\n",
        "criterion_ce = nn.CrossEntropyLoss()\n",
        "criterion_domain = nn.BCEWithLogitsLoss()\n",
        "lambda_grl = 0.25\n",
        "num_epochs = 6\n",
        "best_model = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    adversarial_model.train()\n",
        "    domain_classifier.train()\n",
        "\n",
        "    running_loss, running_ce, running_da = 0.0, 0.0, 0.0\n",
        "    correct_s, correct_t, total_s, total_t = 0, 0, 0, 0\n",
        "\n",
        "    # Iterate over source and target data loaders\n",
        "    for (x_s, y_s), (x_t, y_t) in zip(source_train_loader, target_train_loader):\n",
        "        x_s, y_s = x_s.to(device), y_s.to(device)\n",
        "        x_t, y_t = x_t.to(device), y_t.to(device)\n",
        "        \n",
        "        y_s = torch.tensor([label_map[int(l)] for l in y_s], device=device)\n",
        "        y_t = torch.tensor([label_map[int(l)] for l in y_t], device=device)\n",
        "\n",
        "        # Forward pass\n",
        "        x = torch.cat([x_s, x_t], dim=0)\n",
        "        domain_labels = torch.cat([\n",
        "            torch.ones(x_s.size(0), 1),\n",
        "            torch.zeros(x_t.size(0), 1)\n",
        "        ], dim=0).to(device)\n",
        "\n",
        "        ## TO DO ##\n",
        "        # Forward pass through the adversarial model. Call `logits` the output of the model and `z` the latent representation.\n",
        "        # Split logits and latent representations between the source and target domains. Call the source latent representation `z_s` and target latent representation `z_t`. Call the logits for source domain `logits_s` and for target domain `logits_t`.\n",
        "\n",
        "        ## TO DO ##\n",
        "        # Compute the losses. Use `criterion_ce` for the cross-entropy loss and `criterion_domain` for the domain classification loss. \n",
        "        # criterion domain should take logits from the domain classifier and domain_labels as inputs.\n",
        "        # compute the total loss `loss_total` as the sum of the cross-entropy loss and the domain classification loss.\n",
        "        \n",
        "        if loss_total.item() < best_loss:\n",
        "            best_loss = loss_total.item()\n",
        "            best_model = copy.deepcopy(adversarial_model.state_dict())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer_DANN.zero_grad()\n",
        "        loss_total.backward()\n",
        "        optimizer_DANN.step()\n",
        "\n",
        "        # Track training stats\n",
        "        _, pred_s = logits_s.max(1)\n",
        "        _, pred_t = logits_t.max(1)\n",
        "        correct_s += pred_s.eq(y_s).sum().item()\n",
        "        correct_t += pred_t.eq(y_t).sum().item()\n",
        "        total_s += y_s.size(0)\n",
        "        total_t += y_t.size(0)\n",
        "\n",
        "        running_loss += loss_total.item()\n",
        "        running_ce += loss_ce.item()\n",
        "        running_da += loss_da.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"CE Loss: {running_ce/len(source_train_loader):.4f}, \"\n",
        "          f\"DA Loss: {running_da/len(source_train_loader):.4f}, \"\n",
        "          f\"Source Acc: {100 * correct_s / total_s:.2f}%, \"\n",
        "          f\"Target Acc: {100 * correct_t / total_t:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "JSZVbbYmB-10",
        "outputId": "6991c4a8-6ed1-4dfe-85c6-08386f9fef30"
      },
      "outputs": [],
      "source": [
        "adversarial_model.load_state_dict(best_model)\n",
        "# Test on source domain (MNIST)\n",
        "source_acc = test_model(adversarial_model, source_test_loader, device)\n",
        "# Test on target domain (MNIST-M)\n",
        "target_acc = test_model(adversarial_model, target_test_loader, device)\n",
        "\n",
        "print(f\"Source Test Accuracy: {source_acc:.2f}%\")\n",
        "print(f\"Target Test Accuracy: {target_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that our adversarial approach was also succesful, though less performant than the distance-based approach. In general, one can find that adversarial approaches can *exceed* distance-based approaches for DA, though requiring much more hyperparameter tuning. We have done none of that, which can explain the discrepancy.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Studying Isomaps\n",
        "\n",
        "We have now trained three models: `CE_only_model`, `DA_model`, and `adversarial_model`, with varying performance on the target domain. These models used the same underlying CNN. It's now interesting to study how these models latent representations differ, and what that can tell us about the success and failure points of the respective training methods.\n",
        "\n",
        "Recall that our latent space $z$ is high-dimensional. In this case, 128-d. To understand any relevant structure *as people*, we will have to project this down to 2D. Luckily, there are a variety of methods designed to do this.\n",
        "\n",
        "One such method is [Isomaps](https://en.wikipedia.org/wiki/Isomap). The isomap method reduces dimensionality by preserving geodesic (manifold) distances between points. First, a neighborhood graph is built using $k$-nearest neighbors. Geodesic distances $D_{ij}$ are then estimated via shortest paths over this graph. Finally, classical MDS is applied to the resulting distance matrix. The embedding is given by the top two eigenvectors of:\n",
        "\n",
        "$$\n",
        "\\mathbf{B} = -\\frac{1}{2} \\mathbf{H} D^2 \\mathbf{H}, \\quad \\text{where } \\mathbf{H} = \\mathbf{I} - \\frac{1}{N} \\mathbf{1}\\mathbf{1}^\\top.\n",
        "$$\n",
        "\n",
        "This gives us a 2D projection that captures the global structure of the latent space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-mjlsvyDWow"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import Isomap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def extract_latents_with_labels(model, source_loader, target_loader, device):\n",
        "    model.eval()\n",
        "    z_all, y_all, domains = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in source_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            _, z = model(imgs)\n",
        "            z_all.append(z.cpu())\n",
        "            y_all.extend([label_map[int(y)] for y in labels])\n",
        "\n",
        "            domains.extend([\"source\"] * len(labels))\n",
        "\n",
        "        for imgs, labels in target_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            _, z = model(imgs)\n",
        "            z_all.append(z.cpu())\n",
        "            y_all.extend([label_map[int(y)] for y in labels])\n",
        "\n",
        "            domains.extend([\"target\"] * len(labels))\n",
        "\n",
        "    z_all = torch.cat(z_all, dim=0)\n",
        "    z_all = torch.tensor(StandardScaler().fit_transform(z_all))\n",
        "    return z_all, np.array(y_all), np.array(domains)\n",
        "\n",
        "def plot_isomap_with_colors_and_markers(z_all, y_all, domains, title, xlim=None, ylim=None, subset=500, ax=None, legend=False):\n",
        "\n",
        "    isomap = Isomap(n_components=2, n_neighbors=20)\n",
        "    z_2d = isomap.fit_transform(z_all.numpy())\n",
        "\n",
        "    indices = np.random.choice(len(z_all), size=subset, replace=False)\n",
        "    z_2d = z_2d[indices]\n",
        "    y_all = y_all[indices]\n",
        "    domains = domains[indices]\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(7, 5))\n",
        "\n",
        "    cmap = plt.cm.get_cmap('tab10', 3)\n",
        "\n",
        "    for digit in range(3):\n",
        "        color = cmap(digit)\n",
        "        for domain, marker in zip([\"source\", \"target\"], [\"o\", \"^\"]):\n",
        "            idx = (y_all == digit) & (domains == domain)\n",
        "            ax.scatter(\n",
        "                z_2d[idx, 0], z_2d[idx, 1],\n",
        "                label=f\"{digit} ({'src' if domain == 'source' else 'tgt'})\",\n",
        "                c=[color], marker=marker, s=15, alpha=0.7\n",
        "            )\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Isomap dim 1\")\n",
        "    ax.set_ylabel(\"Isomap dim 2\")\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)\n",
        "    if legend:\n",
        "        ax.legend(markerscale=1.5, fontsize=9, loc='center left', bbox_to_anchor=(1.02, 0.5))\n",
        "\n",
        "    if ax is None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# CE-only model\n",
        "z_ce, y_ce, d_ce = extract_latents_with_labels(CE_only_model, source_test_loader, target_test_loader, device)\n",
        "plot_isomap_with_colors_and_markers(z_ce, y_ce, d_ce, \"Isomap: CE Only\", ax=axes[0], legend=False)\n",
        "\n",
        "# MMD-based model\n",
        "z_da, y_da, d_da = extract_latents_with_labels(DA_model, source_test_loader, target_test_loader, device)\n",
        "plot_isomap_with_colors_and_markers(z_da, y_da, d_da, \"Isomap: MMD Domain Adaptation\", ax=axes[1], legend=False)\n",
        "\n",
        "# Adversarial model\n",
        "z_adv, y_adv, d_adv = extract_latents_with_labels(adversarial_model, source_test_loader, target_test_loader, device)\n",
        "plot_isomap_with_colors_and_markers(z_adv, y_adv, d_adv, \"Isomap: Adversarial Domain Adaptation\", ax=axes[2], legend=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there is considerable overlap in the CE-only model. We still do see a slight improvement in the latent spaces that include DA. And this, in fact, does make sense. We wouldn't expect perfect, nice clusters for each class as the target domain performance is still not perfectly aligned with the source. If you play around with things more and get the source domain performance on-par with the target domain, you should see some nice isomaps like the ones visualized in Figure 5 [here](https://arxiv.org/pdf/2501.14048v2). \n",
        "\n",
        "---\n",
        "After finishing and if you want to play around with making these results better, to recap here are a few options for things you can do:\n",
        "\n",
        "* Make an even deeper CNN model with the components that generally help model convergence/generalization (dropout, batch normalization, pooling).\n",
        "* Test the existing model (and/or a deeper one) on the full MNIST and MNIST-M datasets with all digits. How does the performance do? How do the isomaps look?\n",
        "* Consider adding L2-regularization, gradient clipping, and/or a learning rate scheduler to stabalize the training.\n",
        "* install `escnn` and construct a dihedral group $D_N$ equivariant model. Study how well the model generalizes compared to the CNN. How does the latent space look for this model?\n",
        "* Play around with different choices of MMD kernel inside `geomloss`. Consider implementing a superposition of Gaussian kernels using `kernel='gaussian'` with varying scales for $\\sigma$. How does the model perform?\n",
        "\n",
        "\n",
        "For a dataset like MNIST, some combination of all of these can get pretty high in accuracy (~77% for MMD and DANN). See the leaderboard [here](https://paperswithcode.com/sota/domain-adaptation-on-mnist-to-mnist-m). Thus concludes this tutorial. Happy coding!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
