{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec25be4d",
   "metadata": {},
   "source": [
    "Probing the Limits of Domain Adaptation\n",
    "---\n",
    "##### Author: [Sneh Pandya](https://snehjp2.github.io)\n",
    "\n",
    "In the last tutorial, we saw an instance of distance-based and adversarial domain adaptation (DA) techniques working to improve the generalization of a NN trained on MNIST and evaluated on MNIST-M. We will now move to a more physical dataset of galaxy morphologies, with the differences between source and target domain due to fundamentally different imaging devices. We will be using the Sinkhorn divergence as our distance metric for DA. \n",
    "\n",
    "<img src=\"https://snehjp2.github.io/images/gaussian_meme.png\" alt=\"gaussian meme\" width=\"600\"/>\n",
    "\n",
    "\n",
    "The particular focus here will be playing around with hyperparameters, illustrating the success and failure points of DA and the importance of finding the right sweet-spot for training. We will end with more sophisticated techniques for implementing DA that can automate-away the need for hyperparameter tuning. To that end we will again consider a subset of the full galaxy dataset to simplify the task. We'll start with the same code imports as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d1cfbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch related imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "import copy\n",
    "\n",
    "# miscellaneous\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Optional\n",
    "import os\n",
    "import random\n",
    "\n",
    "# geomloss provides distance measure that are torch/CUDA compatible. \n",
    "# uncomment line below to install it, if you don't have it installed.\n",
    "\n",
    "# !pip install -q geomloss\n",
    "\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # For Python's hash seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Optional: enforce deterministic algorithms where possible\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    \n",
    "set_all_seeds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374197f",
   "metadata": {},
   "source": [
    "Problem Setup\n",
    "---\n",
    "\n",
    "Our problem setup follows similarly from Tutorial 1. We will being using the Galaxy Zoo (GZ) Evo dataset for this tutorial. GZ is a citizen science project that labels galaxy images through online participation. GZ Evo combines labeled image datasets across several surveys and iterations of GZ. Within GZ Evo, we use the GZ2 Dataset from the [Sloan Digital Sky Survey](https://www.sdss4.org) (SDSS) [Willett et al., 2013] as the source, and a GZ [Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging Surveys](https://www.legacysurvey.org) dataset that combines observations from the DESI Imaging Surveys (DECals, MzLS, BASS, DES) as the target domain.\n",
    "\n",
    "- **Neural Network**:  \n",
    "Let $f_\\theta$ denote a neural network classifier with parameters $\\theta$, which maps an input image $x \\in \\mathbb{R}^n$ to predicted class probabilities $\\hat{y} = f_\\theta(x) \\in \\mathbb{R}^K$. The network is decomposed into a feature extractor $\\phi_\\theta : \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ and a classifier head $g_\\theta : \\mathbb{R}^d \\rightarrow \\mathbb{R}^K$, such that $f_\\theta(x) = \\text{softmax}(g_\\theta(\\phi_\\theta(x)))$.  \n",
    "We define the **latent representation** $z = \\phi_\\theta(x)$ as the output of the final hidden layer (before the logits). This representation (also called the *latent vector* or *latent space*) will be used for domain alignment.\n",
    "\n",
    "- **Source domain dataset**:  \n",
    "$\\mathcal{D}_s = \\{(x_s^{(i)}, y_s^{(i)})\\}_{i=1}^{N_s}$,  \n",
    "where $x_s^{(i)} \\sim p_s(x)$ are galaxy morphology observations from SDSS, and $y_s^{(i)} \\in \\{0, 1, \\dots, 5\\}$ are the corresponding morphology labels. More information on the dataset can be found in Section 3 [here](https://arxiv.org/pdf/2501.14048).\n",
    "\n",
    "- **Target domain dataset**:  \n",
    "$\\mathcal{D}_t = \\{x_t^{(j)}\\}_{j=1}^{N_t}$,  \n",
    "where $x_t^{(j)} \\sim p_t(x)$ are unlabeled galaxy morphology observations from the DESI Legacy Imaging Surveys. The included morphologies are the same in both datasets. More information on the dataset can be found in Section 3 [here](https://arxiv.org/pdf/2501.14048).\n",
    "We assume the label distributions are aligned, i.e., $p_s(y|x) = p_t(y|x)$, but the input distributions differ: $p_s(x) \\neq p_t(x)$.  \n",
    "Our goal is to adapt $f_\\theta$ using **only labeled source data** and **unlabeled target data**, so that it performs well on the target domain.\n",
    "\n",
    "- **Classification loss**:  \n",
    "On the source domain, we minimize the supervised cross-entropy loss between predicted and true labels:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CE}}(\\theta) = -\\frac{1}{N_s} \\sum_{i=1}^{N_s} \\log f_\\theta^{(y_s^{(i)})}(x_s^{(i)}),\n",
    "$$\n",
    "\n",
    "where $f_\\theta^{(k)}(x)$ denotes the predicted probability for class $k$. We proceed similarly to the previous tutorial and consider a subset of the data, so we set $K = 4$ as the number of classes.\n",
    "\n",
    "---\n",
    "Let's start by defining our necessary ingredients, starting with the data. We will download the [Galaxy Zoo Evo Dataset](https://huggingface.co/datasets/mwalmsley/gz_evo) from [this link](https://zenodo.org/uploads/14583107) (click the galaxy_dataset.zip). The dataset is 2.35 GB in size; make sure to place it in the `data/` directory for the code below to run correctly.\n",
    "\n",
    "Recall that images are typically normalized to be in the range [-1,1] for gradient stabilitiy during training. This is done via [z-score normalization](https://en.wikipedia.org/wiki/Standard_score). I've precalculated the means and standard deviations of the datasets for you. Feel free to check this yourself! This is an often not-discussed/overlooked aspect of training NNs, but data normalization is **immensely important**. Lets start by defining our dataset class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "300e09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GZEvo(Dataset):\n",
    "    \"\"\"Dataset class for the Galaxy Zoo Evolution dataset.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input data.\n",
    "        output_path (Optional[str], optional): Path to the output data. Defaults to None.\n",
    "        transform (Optional[Callable], optional): Transform to apply to the data. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        output_path: Optional[str] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.transform = transform\n",
    "\n",
    "        try:\n",
    "            self.img = np.load(self.input_path)\n",
    "            self.label = np.load(self.output_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Error loading data from {input_path} and {output_path}: {e}\"\n",
    "            )\n",
    "\n",
    "        self.length = len(self.img)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = self.img[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.tensor(self.label[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b283ad7",
   "metadata": {},
   "source": [
    "Like with MNIST-M, we need to be careful about image normalization. I've precalculated the dataset statistics for you again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50b2823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the subsets for training and testing\n",
    "train_subset_size = 5000\n",
    "test_subset_size = 1000\n",
    "\n",
    "# Precomputed means and standard deviations for normalization\n",
    "sdss_mean = (0.0439, 0.0388, 0.0289)\n",
    "sdss_std = (0.0816, 0.0687, 0.0590)\n",
    "desi_mean = (0.1040, 0.0971, 0.0951)\n",
    "desi_std = (0.0835, 0.0789, 0.0754)\n",
    "\n",
    "# Define the target classes we want to keep in the dataset\n",
    "target_classes = [0, 1, 4, 2]\n",
    "\n",
    "# Image transformations for the datasets\n",
    "sdss_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=sdss_mean, std=sdss_std),\n",
    "                transforms.Resize((28, 28)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "desi_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=desi_mean, std=desi_std),\n",
    "                transforms.Resize((28, 28)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# Loading the datasets\n",
    "source_train_dataset = GZEvo(\n",
    "    input_path=\"./data/galaxy_dataset/galaxy_dataset/train/x_train_sdss.npy\",\n",
    "    output_path=\"./data/galaxy_dataset/galaxy_dataset/train/y_train_sdss.npy\",\n",
    "    transform=sdss_transform\n",
    ")\n",
    "\n",
    "target_train_dataset = GZEvo(\n",
    "    input_path=\"./data/galaxy_dataset/galaxy_dataset/train/x_train_desi.npy\",\n",
    "    output_path=\"./data/galaxy_dataset/galaxy_dataset/train/y_train_desi.npy\",\n",
    "    transform=desi_transform\n",
    ")\n",
    "\n",
    "source_test_dataset = GZEvo(\n",
    "    input_path=\"./data/galaxy_dataset/galaxy_dataset/test/x_test_sdss.npy\",\n",
    "    output_path=\"./data/galaxy_dataset/galaxy_dataset/test/y_test_sdss.npy\",\n",
    "    transform=sdss_transform\n",
    ")\n",
    "\n",
    "target_test_dataset = GZEvo(\n",
    "    input_path=\"./data/galaxy_dataset/galaxy_dataset/test/x_test_desi.npy\",\n",
    "    output_path=\"./data/galaxy_dataset/galaxy_dataset/test/y_test_desi.npy\",\n",
    "    transform=desi_transform\n",
    ")\n",
    "\n",
    "# Only keep the target classes in the datasets\n",
    "def filter_indices(dataset, allowed_labels):\n",
    "    return [i for i, (_, label) in enumerate(dataset) if label in allowed_labels]\n",
    "\n",
    "filtered_train_indices = filter_indices(source_train_dataset, target_classes)\n",
    "filtered_test_indices = filter_indices(source_test_dataset, target_classes)\n",
    "\n",
    "source_train_indices = torch.tensor(filtered_train_indices)[torch.randperm(len(filtered_train_indices))[:train_subset_size]]\n",
    "source_test_indices = torch.tensor(filtered_test_indices)[torch.randperm(len(filtered_test_indices))[:test_subset_size]]\n",
    "target_train_indices = filter_indices(target_train_dataset, target_classes)\n",
    "target_test_indices = filter_indices(target_test_dataset, target_classes)\n",
    "\n",
    "source_train_dataset = Subset(source_train_dataset, source_train_indices)\n",
    "source_test_dataset = Subset(source_test_dataset, source_test_indices)\n",
    "target_train_dataset = Subset(target_train_dataset, target_train_indices)\n",
    "target_test_dataset = Subset(target_test_dataset, target_test_indices)\n",
    "\n",
    "# Create DataLoaders for the datasets\n",
    "source_train_loader = DataLoader(\n",
    "    source_train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "target_train_loader = DataLoader(\n",
    "    target_train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "source_test_loader = DataLoader(\n",
    "    source_test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "target_test_loader = DataLoader(\n",
    "    target_test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4388b0",
   "metadata": {},
   "source": [
    "Now that the data is downloaded, lets just sanity check that the images look like what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "692bd909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_dict = {'barred_spiral': 0, \n",
    "               'edge_on_disk': 1, \n",
    "               'featured_without_bar_or_spiral': 2, \n",
    "               'smooth_cigar': 3, \n",
    "               'smooth_round': 4,\n",
    "               'unbarred_spiral': 5\n",
    "            }\n",
    "labels_dict = {v: k for k, v in labels_dict.items()}\n",
    "\n",
    "def get_examples_by_label(dataset):\n",
    "    label_to_img = {}\n",
    "    for img, label in dataset:\n",
    "        label = int(label)\n",
    "        if label not in label_to_img:\n",
    "            label_to_img[label] = img\n",
    "        if len(label_to_img) == 6:\n",
    "            break\n",
    "    return label_to_img\n",
    "\n",
    "# Undo image normalization\n",
    "def unnormalize(img, mean, std):\n",
    "    img = img.clone()\n",
    "    for c in range(img.shape[0]):\n",
    "        img[c] = img[c] * std[c] + mean[c]\n",
    "    return img\n",
    "\n",
    "# Collect the examples\n",
    "sdss_galaxy = get_examples_by_label(source_train_dataset)\n",
    "desi_galaxy = get_examples_by_label(target_train_dataset)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "for idx, image in enumerate(target_classes):\n",
    "\n",
    "    image_sdss = unnormalize(sdss_galaxy[image], sdss_mean, sdss_std)\n",
    "    axes[0, idx].imshow(image_sdss.permute(1, 2, 0).numpy())\n",
    "    axes[0, idx].set_title(f\"{labels_dict[idx]}\", fontsize=10)\n",
    "    axes[0, idx].axis(\"off\")\n",
    "\n",
    "    image_desi = unnormalize(desi_galaxy[image], desi_mean, desi_std)\n",
    "    axes[1, idx].imshow(image_desi.permute(1, 2, 0).numpy())\n",
    "    axes[1, idx].set_title(f\"{labels_dict[idx]}\", fontsize=10)\n",
    "    axes[1, idx].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"SDSS\", fontsize=12)\n",
    "axes[1, 0].set_ylabel(\"DESI\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9ce8c",
   "metadata": {},
   "source": [
    "Pretty pixelated! If you wanna just look at some pretty galaxy pictures, just visualize the images again without resizing. \n",
    "\n",
    "Ok, so the images look good, now lets just check that the shapes are the same for training, and that the z-score normalization statistics look ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3feb0521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDSS image shape: torch.Size([1, 3, 28, 28])\n",
      "DESI image shape: torch.Size([1, 3, 28, 28])\n",
      "SDSS image stats: tensor(-0.0002, dtype=torch.float64) tensor(0.7648, dtype=torch.float64)\n",
      "DESI image stats: tensor(0.0263, dtype=torch.float64) tensor(1.1457, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Get a sample image from each dataset\n",
    "sdss_image = next(iter(DataLoader(source_test_dataset, batch_size=1)))[0]\n",
    "desi_image = next(iter(DataLoader(source_train_dataset, batch_size=1)))[0]\n",
    "\n",
    "# Print the shapes\n",
    "print(\"SDSS image shape:\", sdss_image.shape) # should be (3, 28, 28)\n",
    "print(\"DESI image shape:\", desi_image.shape) # should be (3, 28, 28)\n",
    "print(\"SDSS image stats:\", torch.mean(sdss_image), torch.std(sdss_image)) # should be close to 0 and 1\n",
    "print(\"DESI image stats:\", torch.mean(desi_image), torch.std(desi_image)) # should be close to 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9987e",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's define our model. We will use the same CNN architecture from the previous tutorial. Since you already wrote the model then, here it is in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32206a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2), \n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.flatten_dim = 128 * 7 * 7\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        z = x.view(x.size(0), -1)\n",
    "        out = self.classifier(z)\n",
    "        return out, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7c027e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:20<00:40, 20.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]  Loss: 0.6802  Accuracy: 75.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:37<00:18, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3]  Loss: 0.4861  Accuracy: 80.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:54<00:00, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3]  Loss: 0.4484  Accuracy: 83.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "CE_only_model = CNN(num_classes=4).to(device)\n",
    "\n",
    "label_map = {0 : 0, 1 : 1, 4 : 2, 2 : 3}\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(CE_only_model.parameters(), lr=1e-3)\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    CE_only_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in source_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.float()\n",
    "        \n",
    "        labels = torch.tensor([label_map[int(l)] for l in labels], device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = CE_only_model(images) ## not using latent z for now\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Model saving\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model = copy.deepcopy(CE_only_model.state_dict())\n",
    "\n",
    "        # Track training stats\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]  Loss: {running_loss/len(source_train_loader):.4f}  Accuracy: {100*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2236f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (SDSS): 81.80%\n",
      "Accuracy on target domain (DESI): 75.51%\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = torch.tensor([label_map[int(l)] for l in labels], device=device)\n",
    "\n",
    "            images = images.float()\n",
    "            outputs, _ = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Test on source domain (SDSS)\n",
    "CE_only_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(CE_only_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (SDSS): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (DESI)\n",
    "target_accuracy = test_model(CE_only_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (DESI): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a602dc",
   "metadata": {},
   "source": [
    "We see that the model has a ~6% discrepancy between the source and target domain. Indeed, despite the galaxy observations coming from fundamentally different imaging instruments, the images dont look *too* different. Can you think about why the two datasets look similar despite coming from SDSS and DESI? Research the photometric filters used in both surveys, and their wavelength coverage. \n",
    "\n",
    "*Stretch Question:* Would you expect galaxy observations from JWST and DESI to look similar? What would be different about constructing a model that generalizes between SDSS and JWST observations? Think about imaging modalities, but also think about what kind of galaxies exist at different redshifts. What kind of domain shift would this be? See tips [here](https://ned.ipac.caltech.edu/level5/March04/Conselice/Conselice3.html) and [here](https://www.stsci.edu/jwst/instrumentation).\n",
    "\n",
    "Now, lets move to domain adaptation and see if we can shrink this performance gap between $\\mathcal{D}_s$ and $\\mathcal{D}_t$. We will again use `geomloss`, but this time using the Sinkhorn Divergence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d305f17",
   "metadata": {},
   "source": [
    "The Sinkhorn Divergence\n",
    "---\n",
    "![Optimal Transport](https://lchizat.github.io/files/UOTvh.gif)\n",
    "\n",
    "The optimal transport (OT) distance is distance measure much like MMD. One notable problem with MMD distances is the user-specified choice of kernel function $k$. There are several principled kernel options (e.g., Gaussian, Laplacian, linear, etc.) with the optimal choice of kernel determined through extensive experimentation. A single kernel may not be optimal, either, and a superposition of kernels may be needed for optimal domain alignment. OT distances avoid this by not requiring a kernel function in computing distances, and have gained popularity in recent years due to *entropic regularization* making the calculation of such distances more manageable. The regularized OT is defined as\n",
    "\n",
    "$$\n",
    "\\text{OT}_\\sigma(\\mu, \\nu) = \\min_{\\gamma \\in U(\\mu, \\nu)} \\left( \\sum_{i,j} \\gamma_{ij} d(z_i, z_j^*)^p + \\sigma H(\\gamma) \\right),\n",
    "$$\n",
    "\n",
    "where $d(z_i, z_j^*)^p$ is the distance between source feature $z_i$ and target feature $z_j^*$. When $p = 1$, this distance becomes the Earth Mover’s Distance [Rubner et al., 1998], and when \\( p = 2 \\), it becomes the quadratic Wasserstein distance.\n",
    "\n",
    "The transport plan $\\gamma \\in U(\\mu, \\nu)$ is a joint probability distribution between $\\mu$ and $\\nu$, where the set of admissible transport plans $U(\\mu, \\nu)$ is defined by the marginal constraints:\n",
    "\n",
    "$$\n",
    "\\sum_j \\gamma_{ij} = \\mu_i, \\quad \\sum_i \\gamma_{ij} = \\nu_j. \\tag{4}\n",
    "$$\n",
    "\n",
    "Much of the expense in the regularized OT problem is finding measures that satisfy these constraints. The entropy $H(\\gamma) = -\\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij}$ regularizes the transport plan $\\gamma$, and $\\sigma$ controls the regularization strength (this is sometimes referred to as the \"blur\" parameter). One limitation of $\\text{OT}_\\sigma$ is that $\\text{OT}_\\sigma(\\mu, \\mu) \\ne 0$, implying a non-zero cost even when transporting a distribution to itself, leading to bias in the measure.\n",
    "\n",
    "To correct this bias, the Sinkhorn divergence $S_\\sigma(\\mu, \\nu)$ is defined as\n",
    "\n",
    "$$\n",
    "S_\\sigma(\\mu, \\nu) = \\text{OT}_\\sigma(\\mu, \\nu) - \\frac{1}{2} \\text{OT}_\\sigma(\\mu, \\mu) - \\frac{1}{2} \\text{OT}_\\sigma(\\nu, \\nu), \\tag{5}\n",
    "$$\n",
    "\n",
    "a linear combination of $\\text{OT}_\\sigma$ terms. It can correct for this bias [Feydy et al., 2018]. As $\\sigma \\to 0$, $S_\\sigma(\\mu, \\nu)$ converges to the (unbiased) optimal transport $\\text{OT}_0$, and as $\\sigma \\to \\infty$, it interpolates towards MMD loss [Feydy et al., 2018]. For small values of $\\sigma$, an unbiased transport plan that still enjoys the benefits of OT-based distances can be constructed. We're now ready for our full loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{CE}}(y_s, \\hat{y}_s) + \\lambda \\cdot S_\\sigma(z, z^*),\n",
    "$$\n",
    "\n",
    "where $\\lambda$ dictates the strength of the DA loss term.\n",
    "\n",
    "A few understanding checks before we move forward:\n",
    "* Double check that the Sinkhorn Divergence is unbiased: $S_\\sigma(\\mu, \\mu) = S_\\sigma(\\nu, \\nu) = 0$. What would happen if you performed DA with a biased measure ($\\text{OT}(\\mu, \\nu)$)?\n",
    "* There is one hyperparameter here. What is the reason for having $\\sigma \\to 0$ vs. $\\sigma \\to \\infty$? Read about [Sinkhorn's algorithm](https://lucyliu-ucsb.github.io/posts/Sinkhorn-algorithm/) and think also about computational complexity / speed.\n",
    "* What would happen when training with $\\lambda \\to 0$. What about $\\lambda = 1$ and $\\lambda \\gg 1$?\n",
    "\n",
    "Our focus for these next code blocks will be the $\\lambda$ term and one technique to finding the optimal balance between the primary learning objective and DA. Lets start with $\\lambda = 0.1$. We will use a fixed value of $\\sigma = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c903675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:33<02:47, 33.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], CE Loss: 0.9172, DA Loss: 67.1225 Source Acc: 72.60%, Target Acc: 73.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [01:08<02:16, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], CE Loss: 0.8528, DA Loss: 1.7546 Source Acc: 72.98%, Target Acc: 74.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [01:38<01:37, 32.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], CE Loss: 0.7179, DA Loss: 1.1282 Source Acc: 73.58%, Target Acc: 73.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [02:09<01:03, 31.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], CE Loss: 0.6132, DA Loss: 0.9010 Source Acc: 76.24%, Target Acc: 76.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [02:40<00:31, 31.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], CE Loss: 0.5553, DA Loss: 0.7335 Source Acc: 77.54%, Target Acc: 78.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:10<00:00, 31.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], CE Loss: 0.5095, DA Loss: 0.6927 Source Acc: 80.00%, Target Acc: 79.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "DA_model = CNN().to(device)\n",
    "\n",
    "num_epochs = 6\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(DA_model.parameters(), lr=1e-3)\n",
    "lambda_DA = 0.1\n",
    "\n",
    "## TO DO ## \n",
    "# Initialize geomloss_fn with Sinkhorn divergence, with the Wasserstein distance cost function and blur ~ 10; tune as needed.\n",
    "# See https://www.kernel-operations.io/geomloss/api/pytorch-api.html for guidance and syntax.\n",
    "geomloss_fn = None\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Training loop for Domain Adaptation\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    DA_model.train()\n",
    "    running_loss, correct_s, correct_t, total_s, total_t = 0.0, 0, 0, 0, 0\n",
    "    running_DA, running_CE = 0.0, 0.0\n",
    "\n",
    "    # Loop over source and target datasets\n",
    "    for (source_imgs, source_labels), (target_imgs, target_labels) in zip(source_train_loader, target_train_loader):\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs, target_labels = target_imgs.to(device), target_labels.to(device)\n",
    "        source_imgs, target_imgs = source_imgs.float(), target_imgs.float()\n",
    "        \n",
    "        source_labels = torch.tensor([label_map[int(l)] for l in source_labels], device=device)\n",
    "        target_labels = torch.tensor([label_map[int(l)] for l in target_labels], device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        combined_imgs = torch.cat([source_imgs, target_imgs], dim=0)\n",
    "        logits, z = DA_model(combined_imgs)\n",
    "\n",
    "        # Isolate latents\n",
    "        z_s, z_t = z[:source_imgs.size(0)], z[source_imgs.size(0):]\n",
    "        logits_s = logits[:source_imgs.size(0)]\n",
    "        logits_t = logits[source_imgs.size(0):]\n",
    "\n",
    "        # Compute losses\n",
    "        ce_loss = criterion(logits_s, source_labels)\n",
    "        DA_loss = geomloss_fn(z_s, z_t) \n",
    "        total_loss = ce_loss + lambda_DA * DA_loss\n",
    "        \n",
    "        # Model saving\n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_model = copy.deepcopy(DA_model.state_dict())\n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training stats\n",
    "        running_loss += total_loss.item()\n",
    "        running_CE += ce_loss.item()\n",
    "        running_DA += DA_loss.item()\n",
    "        _, predicted_s = logits_s.max(1)\n",
    "        total_s += source_labels.size(0)\n",
    "        correct_s += predicted_s.eq(source_labels).sum().item()\n",
    "        _, predicted_t = logits_t.max(1)\n",
    "        total_t += target_labels.size(0)\n",
    "        correct_t += predicted_t.eq(target_labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CE Loss: {running_CE/len(source_train_loader):.4f}, DA Loss: {running_DA/len(source_train_loader):.4f} Source Acc: {100*correct_s/total_s:.2f}%, Target Acc: {100*correct_t/total_t:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15d46661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (SDSS): 80.10%\n",
      "Accuracy on target domain (DESI): 79.70%\n"
     ]
    }
   ],
   "source": [
    "# Test on source domain (SDSS)\n",
    "DA_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(DA_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (SDSS): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (DESI)\n",
    "target_accuracy = test_model(DA_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (DESI): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f391134",
   "metadata": {},
   "source": [
    "Ok, so some improvement in the target domain here. But also, notice that the source domain performance dropped! In this case, the DA term is probably slightly overweighed. Just to send the point home, let's then test with a much larger $\\lambda$ value ($\\lambda \\gg 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "90d69beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:35<02:55, 35.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], CE Loss: 0.9104, DA Loss: 53.7946 Source Acc: 72.14%, Target Acc: 73.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [01:10<02:21, 35.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], CE Loss: 0.8809, DA Loss: 1.6699 Source Acc: 72.96%, Target Acc: 74.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [01:49<01:50, 36.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], CE Loss: 0.8671, DA Loss: 0.7259 Source Acc: 73.00%, Target Acc: 73.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [02:24<01:12, 36.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], CE Loss: 0.8474, DA Loss: 0.5262 Source Acc: 73.24%, Target Acc: 74.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [02:57<00:35, 35.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], CE Loss: 0.8530, DA Loss: 0.4166 Source Acc: 73.08%, Target Acc: 74.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:35<00:00, 35.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], CE Loss: 0.8488, DA Loss: 0.3064 Source Acc: 73.20%, Target Acc: 73.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "DA_model = CNN().to(device)\n",
    "\n",
    "num_epochs = 6\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(DA_model.parameters(), lr=1e-3)\n",
    "lambda_DA = 1000\n",
    "geomloss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=10)\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Training loop for Domain Adaptation\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    DA_model.train()\n",
    "    running_loss, correct_s, correct_t, total_s, total_t = 0.0, 0, 0, 0, 0\n",
    "    running_DA, running_CE = 0.0, 0.0\n",
    "\n",
    "    # Loop over source and target datasets\n",
    "    for (source_imgs, source_labels), (target_imgs, target_labels) in zip(source_train_loader, target_train_loader):\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs, target_labels = target_imgs.to(device), target_labels.to(device)\n",
    "        source_imgs, target_imgs = source_imgs.float(), target_imgs.float()\n",
    "        \n",
    "        source_labels = torch.tensor([label_map[int(l)] for l in source_labels], device=device)\n",
    "        target_labels = torch.tensor([label_map[int(l)] for l in target_labels], device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        combined_imgs = torch.cat([source_imgs, target_imgs], dim=0)\n",
    "        logits, z = DA_model(combined_imgs)\n",
    "\n",
    "        # Split latents\n",
    "        z_s, z_t = z[:source_imgs.size(0)], z[source_imgs.size(0):]\n",
    "        logits_s = logits[:source_imgs.size(0)]\n",
    "        logits_t = logits[source_imgs.size(0):]\n",
    "\n",
    "        # Compute losses\n",
    "        ce_loss = criterion(logits_s, source_labels)\n",
    "        DA_loss = geomloss_fn(z_s, z_t) \n",
    "        total_loss = ce_loss + lambda_DA * DA_loss\n",
    "        \n",
    "        # Model saving\n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_model = copy.deepcopy(DA_model.state_dict())\n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training stats\n",
    "        running_loss += total_loss.item()\n",
    "        running_CE += ce_loss.item()\n",
    "        running_DA += DA_loss.item()\n",
    "        _, predicted_s = logits_s.max(1)\n",
    "        total_s += source_labels.size(0)\n",
    "        correct_s += predicted_s.eq(source_labels).sum().item()\n",
    "        _, predicted_t = logits_t.max(1)\n",
    "        total_t += target_labels.size(0)\n",
    "        correct_t += predicted_t.eq(target_labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CE Loss: {running_CE/len(source_train_loader):.4f}, DA Loss: {running_DA/len(source_train_loader):.4f} Source Acc: {100*correct_s/total_s:.2f}%, Target Acc: {100*correct_t/total_t:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b483248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (SDSS): 75.00%\n",
      "Accuracy on target domain (DESI): 75.79%\n"
     ]
    }
   ],
   "source": [
    "# Test on source domain (SDSS)\n",
    "DA_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(DA_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (SDSS): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (DESI)\n",
    "target_accuracy = test_model(DA_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (DESI): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117d60e",
   "metadata": {},
   "source": [
    "The source domain performance dropped significantly, and the target domain performance is also low. Keep in mind that we would not expect a model trained with DA to outperform the source domain. So while we must focus on domain alignment, it shouldn't come at the expense of the primarily learning objective, whether it be classification or regression.\n",
    "\n",
    "A reasonable guess would then be that $\\lambda$ should be less than 1, but it is hard to guess a priori whether it should be $\\mathcal{O}(0.1)$, $\\mathcal{O}(0.01)$, etc. To be sure, we'd have to sample $\\lambda \\sim \\text{LogUniform}$ and then refine from there, which can be difficult. Even then, that would only apply for this dataset, and not MNIST in Tutorial 1, or any other dataset. \n",
    "\n",
    "So how can we be certain we're doing things optimally? What one would usually think to do in this case is some sort of search, trying out different values of $\\lambda$ within an appropriate range and seeing what works best. But even then, the results could be sensitive to the choice of seed, so you should marginalize over multiple realizations and make it a grid search. But also, we haven't even thought about what to do with $\\sigma$, which is important as well, so maybe a cube search...?\n",
    "\n",
    "I think you see where this is going. There's a lot of things that are important here. Here, I'll talk about a *trainable* way to avoid the $\\lambda$ search, using an idea from Bayesian inference. See [this paper](https://arxiv.org/abs/1705.07115).\n",
    "\n",
    "The overall idea is two introduce trainable coefficients for the two loss terms. Let us introduce two trainable scalar parameters $\\eta_1$ and $\\eta_2$, and define the following loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2 \\eta_1^2} \\mathcal{L}_\\text{CE} + \\frac{1}{2 \\eta_2^2} \\mathcal{L}_\\text{DA} + \\log(|\\eta_1 \\eta_2|) \n",
    "$$\n",
    "\n",
    "These values can then be optimized jointly with the model parameters, avoiding the need for a user-specified weighting for the loss terms.\n",
    "\n",
    "Let's think about this loss function before proceeding: \n",
    "\n",
    "* What happens to each loss term as its corresponding $\\eta_i \\to 0$.\n",
    "* What does the third term $\\log(|\\eta_1 \\eta_2|)$ do?\n",
    "* We saw that strongly preffering $\\mathcal{L}_\\text{DA}$ over $\\mathcal{L}_\\text{CE}$ will result in bad performance, and that we probably want $\\eta_2^{-2} < 2$. This trainable prescription does not know a priori that doing so is bad. Is there any way to enforce this? *Hint*: Think about what range the parameters should be in, initializing them properly and governing their time evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33780c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:35<02:57, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], CE Loss: 0.8211, DA Loss: 53.8375 Source Acc: 72.72%, Target Acc: 74.25% eta_1: 0.15483352541923523 eta_2: 1.0123703479766846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [01:11<02:23, 35.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], CE Loss: 0.6194, DA Loss: 1.8429 Source Acc: 76.04%, Target Acc: 78.26% eta_1: 0.18175508081912994 eta_2: 1.012046217918396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [01:44<01:44, 34.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], CE Loss: 0.5457, DA Loss: 1.0786 Source Acc: 79.74%, Target Acc: 81.63% eta_1: 0.20137275755405426 eta_2: 1.0113070011138916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [02:17<01:07, 33.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], CE Loss: 0.5062, DA Loss: 0.7053 Source Acc: 81.44%, Target Acc: 82.28% eta_1: 0.21748045086860657 eta_2: 1.0102862119674683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [02:50<00:33, 33.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], CE Loss: 0.4727, DA Loss: 0.4756 Source Acc: 83.42%, Target Acc: 82.61% eta_1: 0.23108986020088196 eta_2: 1.009029507637024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:26<00:00, 34.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], CE Loss: 0.4436, DA Loss: 0.4301 Source Acc: 83.48%, Target Acc: 82.77% eta_1: 0.24316373467445374 eta_2: 1.0076370239257812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "DA_model = CNN().to(device)\n",
    "\n",
    "num_epochs = 6\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(DA_model.parameters(), lr=1e-3)\n",
    "geomloss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=10)\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "## TO DO ##\n",
    "# Initialize trainable coefficients eta_1 and eta_2 using nn.Parameter. \n",
    "# I recommend starting with eta_1 = 0.1 and eta_2 = 1.0, both requiring gradients. Make sure you understand why they are initialized differently.\n",
    "eta_1 = None\n",
    "eta_2 = None\n",
    "\n",
    "## TO DO ##\n",
    "# The optimizer naively will only optimize the model parameters. Add the trainable coefficients eta_1 and eta_2 to the optimizer using `add_param_group`\n",
    "...\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    DA_model.train()\n",
    "    running_loss, correct_s, correct_t, total_s, total_t = 0.0, 0, 0, 0, 0\n",
    "    running_DA, running_CE = 0.0, 0.0\n",
    "\n",
    "    # Loop over source and target datasets\n",
    "    for (source_imgs, source_labels), (target_imgs, target_labels) in zip(source_train_loader, target_train_loader):\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs, target_labels = target_imgs.to(device), target_labels.to(device)\n",
    "        source_imgs, target_imgs = source_imgs.float(), target_imgs.float()\n",
    "\n",
    "        source_labels = torch.tensor([label_map[int(l)] for l in source_labels], device=device)\n",
    "        target_labels = torch.tensor([label_map[int(l)] for l in target_labels], device=device)\n",
    "        \n",
    "        # Forward pass\n",
    "        combined_imgs = torch.cat([source_imgs, target_imgs], dim=0)\n",
    "        logits, z = DA_model(combined_imgs)\n",
    "\n",
    "        # Split latents and outputs\n",
    "        z_s, z_t = z[:source_imgs.size(0)], z[source_imgs.size(0):]\n",
    "        logits_s = logits[:source_imgs.size(0)]\n",
    "        logits_t = logits[source_imgs.size(0):]\n",
    "\n",
    "        # Compute losses\n",
    "        ce_loss = criterion(logits_s, source_labels)\n",
    "        DA_loss = geomloss_fn(z_s, z_t) \n",
    "        ## TO DO ##\n",
    "        # Compute the total loss using the trainable coefficients eta_1 and eta_2 and the function described above.\n",
    "        total_loss = None\n",
    "        \n",
    "        # Model saving\n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_model = copy.deepcopy(DA_model.state_dict())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(DA_model.parameters(), max_norm=10.0)\n",
    "        \n",
    "        ## TO DO ##\n",
    "        # Clamp eta_1 and eta_2 using x.data.clamp for x \\in {eta_1, eta_2} as discussed above. I recommend letting eta_1 be at least 1e-3 and eta_2 be at least 0.25 * eta_1.\n",
    "        ...\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training stats\n",
    "        running_loss += total_loss.item()\n",
    "        running_CE += ce_loss.item()\n",
    "        running_DA += DA_loss.item()\n",
    "        _, predicted_s = logits_s.max(1)\n",
    "        total_s += source_labels.size(0)\n",
    "        correct_s += predicted_s.eq(source_labels).sum().item()\n",
    "        _, predicted_t = logits_t.max(1)\n",
    "        total_t += target_labels.size(0)\n",
    "        correct_t += predicted_t.eq(target_labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CE Loss: {running_CE/len(source_train_loader):.4f}, DA Loss: {running_DA/len(source_train_loader):.4f} Source Acc: {100*correct_s/total_s:.2f}%, Target Acc: {100*correct_t/total_t:.2f}%\", \"eta_1:\", eta_1.item(), \"eta_2:\", eta_2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47051e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (SDSS): 84.40%\n",
      "Accuracy on target domain (DESI): 84.39%\n"
     ]
    }
   ],
   "source": [
    "# Test on source domain (SDSS)\n",
    "DA_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(DA_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (SDSS): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (DESI)\n",
    "target_accuracy = test_model(DA_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (DESI): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d26b64",
   "metadata": {},
   "source": [
    "In this case you've seen that we've improved both the source and target performance (compared to no-DA), though slightly, without any grid search whatsoever! This trainable prescription largely avoided the need to tune these hyperparameters. Keep in mind we did use some inductive bias: we initialized the $\\eta_i$ differently (think as to why we did this), and clipped them in some reasonable way. This was simply to give the parameters a nudge in the right direction.\n",
    "\n",
    "The last thing we want to study is our treatment of $\\sigma$. In the last experiments, we used a fixed value of $\\sigma = 10$, but now lets think about the assumption that a single value of $\\sigma$ is optimal. It may not be.\n",
    "\n",
    "Keep in mind that as $\\sigma \\to 0$ the Sinkhorn plan interpolates closer to $\\text{OT}_0$, and $\\sigma \\to \\infty$ interpolates closer to MMD. Further, MMD is cheaper to compute than $\\text{OT}_0$ (fewer Sinkhorn iterations), but $\\text{OT}_0$ is more accurate. Even then, as $\\sigma$ gets smaller that means more Sinkhorn iterations, which can lead to instability (and bias). So how can we optimize computational efficiency and domain alignment accuracy? \n",
    "\n",
    "Let's now try a simple annealing scheme, so that $\\sigma$ is larger at the beginning of training, when the latent spaces are very misaligned, and smaller at the end, when we want to really focus on domain alignment.\n",
    "\n",
    "Consider the $\\sigma$ scheduler given by:\n",
    "\n",
    "$$\n",
    "\\sigma = 10 * 0.4^\\ell \\quad \\text{where} \\; \\ell \\; \\text{is epoch number}\n",
    "$$\n",
    "\n",
    "We will test this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed88c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:38<03:14, 38.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], CE Loss: 0.8582, DA Loss: 45.9193 Source Acc: 72.42%, Target Acc: 73.64% eta_1: 0.15499429404735565 eta_2: 1.0114336013793945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [01:16<02:31, 37.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], CE Loss: 0.6069, DA Loss: 1.7776 Source Acc: 75.98%, Target Acc: 76.66% eta_1: 0.1802518665790558 eta_2: 1.0110665559768677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [01:49<01:47, 36.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], CE Loss: 0.4972, DA Loss: 0.9364 Source Acc: 81.48%, Target Acc: 80.64% eta_1: 0.19823439419269562 eta_2: 1.010250210762024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [02:25<01:11, 35.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], CE Loss: 0.4988, DA Loss: 0.9909 Source Acc: 81.42%, Target Acc: 80.85% eta_1: 0.21373434364795685 eta_2: 1.0094103813171387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [02:58<00:34, 34.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], CE Loss: 0.4750, DA Loss: 1.0282 Source Acc: 82.24%, Target Acc: 80.99% eta_1: 0.22826483845710754 eta_2: 1.0086525678634644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:31<00:00, 35.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], CE Loss: 0.4404, DA Loss: 0.8600 Source Acc: 83.44%, Target Acc: 80.26% eta_1: 0.2398659884929657 eta_2: 1.0076483488082886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "DA_model = CNN().to(device)\n",
    "\n",
    "num_epochs = 6\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(DA_model.parameters(), lr=1e-3)\n",
    "geomloss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=10) # GeomLoss with Sinkhorn distance, p=2, blur ~ σ; tune as needed\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "## initialize trainable coefficients\n",
    "eta_1 = nn.Parameter(torch.tensor(0.1, requires_grad=True))\n",
    "eta_2 = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
    "\n",
    "optimizer.add_param_group({\"params\": [eta_1, eta_2]})\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    DA_model.train()\n",
    "    running_loss, correct_s, correct_t, total_s, total_t = 0.0, 0, 0, 0, 0\n",
    "    running_DA, running_CE = 0.0, 0.0\n",
    "\n",
    "    # Loop over source and target datasets\n",
    "    for (source_imgs, source_labels), (target_imgs, target_labels) in zip(source_train_loader, target_train_loader):\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs, target_labels = target_imgs.to(device), target_labels.to(device)\n",
    "        source_imgs, target_imgs = source_imgs.float(), target_imgs.float()\n",
    "        \n",
    "        source_labels = torch.tensor([label_map[int(l)] for l in source_labels], device=device)\n",
    "        target_labels = torch.tensor([label_map[int(l)] for l in target_labels], device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        combined_imgs = torch.cat([source_imgs, target_imgs], dim=0)\n",
    "        logits, z = DA_model(combined_imgs)\n",
    "\n",
    "        # Split latents and outputs\n",
    "        z_s, z_t = z[:source_imgs.size(0)], z[source_imgs.size(0):]\n",
    "        logits_s = logits[:source_imgs.size(0)]\n",
    "        logits_t = logits[source_imgs.size(0):]\n",
    "        \n",
    "        ## TO DO ##\n",
    "        # Compute dynamic blur based on epoch number using the equation above.\n",
    "        dynamic_blur = None\n",
    "\n",
    "        # Compute losses\n",
    "        ce_loss = criterion(logits_s, source_labels)\n",
    "        DA_loss = geomloss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=dynamic_blur)(z_s, z_t)\n",
    "        total_loss = (2 * eta_1**2)**-1 * ce_loss + (2 * eta_2**2)**-1 * DA_loss + torch.log(eta_1 * eta_2)\n",
    "        \n",
    "        # Model saving\n",
    "        if total_loss.item() < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_model = copy.deepcopy(DA_model.state_dict())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(DA_model.parameters(), max_norm=10.0)\n",
    "        eta_1.data.clamp_(min=1e-3)\n",
    "        eta_2.data.clamp_(min=0.25*eta_1.data.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training stats\n",
    "        running_loss += total_loss.item()\n",
    "        running_CE += ce_loss.item()\n",
    "        running_DA += DA_loss.item()\n",
    "        _, predicted_s = logits_s.max(1)\n",
    "        total_s += source_labels.size(0)\n",
    "        correct_s += predicted_s.eq(source_labels).sum().item()\n",
    "        _, predicted_t = logits_t.max(1)\n",
    "        total_t += target_labels.size(0)\n",
    "        correct_t += predicted_t.eq(target_labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CE Loss: {running_CE/len(source_train_loader):.4f}, DA Loss: {running_DA/len(source_train_loader):.4f} Source Acc: {100*correct_s/total_s:.2f}%, Target Acc: {100*correct_t/total_t:.2f}%\", \"eta_1:\", eta_1.item(), \"eta_2:\", eta_2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27621377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on source domain (SDSS): 86.30%\n",
      "Accuracy on target domain (DESI): 83.15%\n"
     ]
    }
   ],
   "source": [
    "# Test on source domain (SDSS)\n",
    "DA_model.load_state_dict(best_model)\n",
    "source_accuracy = test_model(DA_model, source_test_loader, device)\n",
    "print(f\"Accuracy on source domain (SDSS): {source_accuracy:.2f}%\")\n",
    "\n",
    "# Test on target domain (DESI)\n",
    "target_accuracy = test_model(DA_model, target_test_loader, device)\n",
    "print(f\"Accuracy on target domain (DESI): {target_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a14c1",
   "metadata": {},
   "source": [
    "The $\\sigma$ scheduler also helps! We can see an increase in the source domain peformance and similar performance in the target domain. There are, of course, numerous options for the time-evolution of $\\sigma$ during training. We have shown in [this paper](https://arxiv.org/abs/2501.14048) that a $\\sigma$ scheduler based on the 2-norm between latent spaces not only results in better performance on both domains, but is also significantly faster. Try to get creative and come up with your own scheduling for $\\sigma$.\n",
    "\n",
    "---\n",
    "\n",
    "If you've finished and want to play around with things further, here's some suggestions:\n",
    "\n",
    "* Track and plot the evolution of the $\\eta_i$ throughout training. What can you say about the training dynamics, and how / when $\\mathcal{L}_\\text{CE}$ and $\\mathcal{L}_\\text{DA}$ are preffered?\n",
    "* Compare the performance of the Sinkhorn divergence with $\\sigma \\to \\infty$ with a Gaussian MMD. You can compute both of these within `geomloss`. Which method performs better? Which is faster?\n",
    "* Visualize the latent space of all these models using isomaps (or UMAP or t-SNE).\n",
    "* Try implementing the scaling for $\\sigma$ as described in Equation 7 [here](https://arxiv.org/pdf/2501.14048). Be creative and think of others! Email me if you find something interesting.\n",
    "\n",
    "Thus concludes this tutorial. Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228cdd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
